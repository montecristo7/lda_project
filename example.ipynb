{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-insulin",
   "metadata": {},
   "source": [
    "An example from https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upper-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    with smart_open.open(url, \"rb\") as file:\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read()\n",
    "                    yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arranged-arbor",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1328debdbc83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Tokenize the documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Split the documents into tokens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "thorough-shaft",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-7fc98b3930a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wordnet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "anticipated-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "front-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "treated-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "appreciated-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disturbed-edward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8644\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "clean-thesaurus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 24 ms, total: 1min 58s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "given-engine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.1401.\n",
      "[([(0.024176192, 'neuron'),\n",
      "   (0.009569491, 'circuit'),\n",
      "   (0.0076048793, 'cell'),\n",
      "   (0.0075144167, 'spike'),\n",
      "   (0.0067892037, 'synaptic'),\n",
      "   (0.0065952875, 'chip'),\n",
      "   (0.0063055987, 'analog'),\n",
      "   (0.0062299604, 'signal'),\n",
      "   (0.005651782, 'voltage'),\n",
      "   (0.0054583726, 'firing'),\n",
      "   (0.004494699, 'response'),\n",
      "   (0.0044115093, 'channel'),\n",
      "   (0.004370286, 'potential'),\n",
      "   (0.004278465, 'frequency'),\n",
      "   (0.004138902, 'fig'),\n",
      "   (0.004025923, 'connection'),\n",
      "   (0.0038297768, 'threshold'),\n",
      "   (0.0038272964, 'synapsis'),\n",
      "   (0.0035814962, 'noise'),\n",
      "   (0.0033613855, 'memory')],\n",
      "  -0.9426353504997164),\n",
      " ([(0.009407897, 'matrix'),\n",
      "   (0.008047599, 'gaussian'),\n",
      "   (0.0059282053, 'density'),\n",
      "   (0.0051058144, 'likelihood'),\n",
      "   (0.005042755, 'mixture'),\n",
      "   (0.00498137, 'prior'),\n",
      "   (0.0048193233, 'solution'),\n",
      "   (0.004728297, 'approximation'),\n",
      "   (0.004545836, 'bayesian'),\n",
      "   (0.004179032, 'component'),\n",
      "   (0.004094893, 'em'),\n",
      "   (0.0038646834, 'posterior'),\n",
      "   (0.0038291612, 'log'),\n",
      "   (0.0036352677, 'estimate'),\n",
      "   (0.003613909, 'field'),\n",
      "   (0.0033228796, 'xi'),\n",
      "   (0.0033113353, 'maximum'),\n",
      "   (0.003075336, 'covariance'),\n",
      "   (0.003061715, 'optimization'),\n",
      "   (0.003000518, 'sample')],\n",
      "  -0.9543817597829766),\n",
      " ([(0.017375052, 'cell'),\n",
      "   (0.010519656, 'visual'),\n",
      "   (0.010239406, 'stimulus'),\n",
      "   (0.009520532, 'object'),\n",
      "   (0.008749112, 'response'),\n",
      "   (0.00850126, 'field'),\n",
      "   (0.007740606, 'activity'),\n",
      "   (0.007725352, 'neuron'),\n",
      "   (0.00762683, 'layer'),\n",
      "   (0.006957441, 'cortex'),\n",
      "   (0.0062665218, 'direction'),\n",
      "   (0.0058810594, 'orientation'),\n",
      "   (0.005175021, 'receptive'),\n",
      "   (0.0051311506, 'cortical'),\n",
      "   (0.005043068, 'connection'),\n",
      "   (0.004936832, 'map'),\n",
      "   (0.0048838384, 'spatial'),\n",
      "   (0.004789204, 'receptive_field'),\n",
      "   (0.0032878604, 'region'),\n",
      "   (0.0032760107, 'motion')],\n",
      "  -0.9903667569250791),\n",
      " ([(0.0072710444, 'bound'),\n",
      "   (0.006716818, 'let'),\n",
      "   (0.0058854404, 'theorem'),\n",
      "   (0.005511652, 'optimal'),\n",
      "   (0.0046523986, 'generalization'),\n",
      "   (0.00426907, 'convergence'),\n",
      "   (0.004111655, 'approximation'),\n",
      "   (0.0038054953, 'threshold'),\n",
      "   (0.0037426518, 'class'),\n",
      "   (0.0035895542, 'proof'),\n",
      "   (0.0034463853, 'hidden'),\n",
      "   (0.0031955785, 'loss'),\n",
      "   (0.003023889, 'dimension'),\n",
      "   (0.0029844102, 'node'),\n",
      "   (0.0029790876, 'finite'),\n",
      "   (0.0029760308, 'policy'),\n",
      "   (0.0028990416, 'gradient'),\n",
      "   (0.0028406968, 'net'),\n",
      "   (0.0027648243, 'layer'),\n",
      "   (0.0027321365, 'condition')],\n",
      "  -1.0522328947233455),\n",
      " ([(0.031473782, 'image'),\n",
      "   (0.0076475497, 'motion'),\n",
      "   (0.007410929, 'signal'),\n",
      "   (0.007075371, 'filter'),\n",
      "   (0.0063729007, 'visual'),\n",
      "   (0.005834771, 'pixel'),\n",
      "   (0.005635957, 'face'),\n",
      "   (0.005484316, 'source'),\n",
      "   (0.0050753155, 'component'),\n",
      "   (0.004993238, 'object'),\n",
      "   (0.0048285783, 'eye'),\n",
      "   (0.0047430885, 'field'),\n",
      "   (0.0047169714, 'movement'),\n",
      "   (0.0046967566, 'velocity'),\n",
      "   (0.004179198, 'vision'),\n",
      "   (0.004164805, 'position'),\n",
      "   (0.0037069956, 'map'),\n",
      "   (0.0034825127, 'scene'),\n",
      "   (0.0034267416, 'scale'),\n",
      "   (0.0033637744, 'noise')],\n",
      "  -1.1237099700781883),\n",
      " ([(0.00965893, 'prediction'),\n",
      "   (0.008826903, 'noise'),\n",
      "   (0.0056457585, 'regression'),\n",
      "   (0.00561951, 'estimate'),\n",
      "   (0.005103139, 'sample'),\n",
      "   (0.0049468386, 'signal'),\n",
      "   (0.004517069, 'hidden'),\n",
      "   (0.0042745406, 'series'),\n",
      "   (0.0042185863, 'validation'),\n",
      "   (0.0037794197, 'expert'),\n",
      "   (0.0037683705, 'variance'),\n",
      "   (0.0036443747, 'training_set'),\n",
      "   (0.0036307415, 'trained'),\n",
      "   (0.0035803858, 'cross'),\n",
      "   (0.0035696384, 'selection'),\n",
      "   (0.003232785, 'layer'),\n",
      "   (0.0029553922, 'net'),\n",
      "   (0.002941719, 'bias'),\n",
      "   (0.0028869924, 'target'),\n",
      "   (0.0028530979, 'time_series')],\n",
      "  -1.127874610090139),\n",
      " ([(0.008619243, 'control'),\n",
      "   (0.0066471654, 'action'),\n",
      "   (0.0065249614, 'memory'),\n",
      "   (0.0043558404, 'dynamic'),\n",
      "   (0.004102747, 'reinforcement'),\n",
      "   (0.004028791, 'rule'),\n",
      "   (0.0037635453, 'controller'),\n",
      "   (0.003721888, 'recurrent'),\n",
      "   (0.00366853, 'net'),\n",
      "   (0.003613971, 'trajectory'),\n",
      "   (0.0035997261, 'hidden'),\n",
      "   (0.0034919744, 'sequence'),\n",
      "   (0.0034693538, 'architecture'),\n",
      "   (0.0032984796, 'learn'),\n",
      "   (0.0031445657, 'robot'),\n",
      "   (0.0028715422, 'environment'),\n",
      "   (0.0028218096, 'policy'),\n",
      "   (0.0027597563, 'learned'),\n",
      "   (0.002677155, 'activation'),\n",
      "   (0.002645788, 'initial')],\n",
      "  -1.1681866642269667),\n",
      " ([(0.016476719, 'recognition'),\n",
      "   (0.015290491, 'speech'),\n",
      "   (0.014552541, 'word'),\n",
      "   (0.011133005, 'hidden'),\n",
      "   (0.0062913415, 'context'),\n",
      "   (0.0062858607, 'sequence'),\n",
      "   (0.0056316406, 'hmm'),\n",
      "   (0.005534832, 'speaker'),\n",
      "   (0.0053977445, 'layer'),\n",
      "   (0.005391189, 'character'),\n",
      "   (0.0049757226, 'mixture'),\n",
      "   (0.00467942, 'trained'),\n",
      "   (0.0045473156, 'frame'),\n",
      "   (0.0044699553, 'architecture'),\n",
      "   (0.00420855, 'net'),\n",
      "   (0.003960958, 'acoustic'),\n",
      "   (0.003832854, 'phoneme'),\n",
      "   (0.003701432, 'hidden_unit'),\n",
      "   (0.0036705688, 'letter'),\n",
      "   (0.00365886, 'class')],\n",
      "  -1.2016023362374522),\n",
      " ([(0.01286798, 'classifier'),\n",
      "   (0.012532556, 'class'),\n",
      "   (0.010992216, 'classification'),\n",
      "   (0.008803224, 'node'),\n",
      "   (0.008351865, 'layer'),\n",
      "   (0.0072750114, 'tree'),\n",
      "   (0.0067777284, 'distance'),\n",
      "   (0.004966051, 'recognition'),\n",
      "   (0.0044875145, 'decision'),\n",
      "   (0.004304788, 'training_set'),\n",
      "   (0.0038274275, 'image'),\n",
      "   (0.0034518044, 'rbf'),\n",
      "   (0.0033159251, 'nearest'),\n",
      "   (0.003072801, 'neighbor'),\n",
      "   (0.003046404, 'machine'),\n",
      "   (0.002856875, 'basis'),\n",
      "   (0.0028433597, 'net'),\n",
      "   (0.0028397283, 'region'),\n",
      "   (0.0028320556, 'back'),\n",
      "   (0.0027942504, 'tangent')],\n",
      "  -1.2651562956013784),\n",
      " ([(0.017200265, 'rule'),\n",
      "   (0.00724904, 'component'),\n",
      "   (0.0058674137, 'gradient'),\n",
      "   (0.005539139, 'map'),\n",
      "   (0.004771441, 'protein'),\n",
      "   (0.0045280494, 'net'),\n",
      "   (0.0044849394, 'pruning'),\n",
      "   (0.0042654527, 'node'),\n",
      "   (0.004093495, 'cell'),\n",
      "   (0.0040342854, 'region'),\n",
      "   (0.0040322794, 'perturbation'),\n",
      "   (0.004018992, 'correlation'),\n",
      "   (0.0036728862, 'ob'),\n",
      "   (0.003664731, 'sequence'),\n",
      "   (0.0033099, 'matrix'),\n",
      "   (0.003187038, 'brain'),\n",
      "   (0.003142174, 'activation'),\n",
      "   (0.0031223174, 'positive'),\n",
      "   (0.003014508, 'chain'),\n",
      "   (0.0030129023, 'principal')],\n",
      "  -1.5750131287944493)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "earlier-celebration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'ab aldsjfld dsjflsdjlkdsf ddsf'), (1, 'fdsalj dlsk fj s ab')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=Dictionary(documents=[['fdsalj dlsk fj s ab','ab aldsjfld dsjflsdjlkdsf ddsf']])\n",
    "list(d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "protected-thomson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
       "\n",
       "Notable instance attributes:\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "token2id : dict of (str, int)\n",
       "    token -> tokenId.\n",
       "id2token : dict of (int, str)\n",
       "    Reverse mapping for token2id, initialized in a lazy manner to save memory (not created until needed).\n",
       "cfs : dict of (int, int)\n",
       "    Collection frequencies: token_id -> how many instances of this token are contained in the documents.\n",
       "dfs : dict of (int, int)\n",
       "    Document frequencies: token_id -> how many documents contain this token.\n",
       "num_docs : int\n",
       "    Number of documents processed.\n",
       "num_pos : int\n",
       "    Total number of corpus positions (number of processed words).\n",
       "num_nnz : int\n",
       "    Total number of non-zeroes in the BOW matrix (sum of the number of unique\n",
       "    words per document over the entire corpus).\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Parameters\n",
       "----------\n",
       "documents : iterable of iterable of str, optional\n",
       "    Documents to be used to initialize the mapping and collect corpus statistics.\n",
       "prune_at : int, optional\n",
       "    Dictionary will try to keep no more than `prune_at` words in its mapping, to limit its RAM\n",
       "    footprint, the correctness is not guaranteed.\n",
       "    Use :meth:`~gensim.corpora.dictionary.Dictionary.filter_extremes` to perform proper filtering.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.corpora import Dictionary\n",
       "    >>>\n",
       "    >>> texts = [['human', 'interface', 'computer']]\n",
       "    >>> dct = Dictionary(texts)  # initialize a Dictionary\n",
       "    >>> dct.add_documents([[\"cat\", \"say\", \"meow\"], [\"dog\"]])  # add more document (extend the vocabulary)\n",
       "    >>> dct.doc2bow([\"dog\", \"computer\", \"non_existent_word\"])\n",
       "    [(0, 1), (6, 1)]\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/gensim/corpora/dictionary.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "christian-dictionary",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f64714510a32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type str)"
     ]
    }
   ],
   "source": [
    "c=chr('C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "buried-classroom",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-305c6b8124d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "c-'0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "greek-concentration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('E')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "heard-session",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "[i*(i+1) for i in range(18)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-certification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
