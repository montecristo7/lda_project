{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "streaming-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import random\n",
    "from scipy.special import gammaln, psi, polygamma\n",
    "from functools import reduce\n",
    "from warnings import warn\n",
    "import utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "outstanding-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLDA(object):\n",
    "    def __init__(self, docs):\n",
    "        self.docs=docs\n",
    "    def lda(self,num_topics):\n",
    "        raise ValueError('Method not implemented.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "uniform-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA2(BaseLDA):\n",
    "    def __init__(self, docs):\n",
    "        self.M=len(docs)\n",
    "        self.vocab=None\n",
    "        self.V=-1\n",
    "        self.topics=None\n",
    "        super().__init__(docs)\n",
    "\n",
    "    def make_vocab_from_docs(self):\n",
    "        \"\"\"\n",
    "        Make a dictionary that contains all words from the docs. The order of words is arbitrary.\n",
    "        docs: iterable of documents\n",
    "        \"\"\"\n",
    "        vocab_words=set()\n",
    "        for doc in self.docs:\n",
    "            doc=doc.lower()\n",
    "            doc=re.sub(r'-',' ',doc)\n",
    "            doc=re.sub(r' +',' ',doc) # turn multiple spaces into a single space\n",
    "            doc=re.sub(r'[^a-z ]','',doc) # remove anything that is not a-z or space\n",
    "            words=set(doc.split())\n",
    "            vocab_words=vocab_words.union(words)\n",
    "            vocab=dict(zip(vocab_words,range(len(vocab_words))))\n",
    "        self.vocab=vocab\n",
    "        self.V=len(vocab)\n",
    "        return vocab\n",
    "    def parse_doc(self,doc,vocab):\n",
    "        \"\"\"\n",
    "        Parse a single document. \n",
    "        Arguments:\n",
    "        doc: document string\n",
    "        vocab: a dictionary that maps words to integers\n",
    "        Output:\n",
    "        A list of tuples, where for each tuple, the first element is a word appeared in the doc, labeled with the integers in the vocab dictionary (the set of $\\tilde{w_n}$), \n",
    "            and the second element is count of the words.\n",
    "        The words that are not in vocab will be ignored.\n",
    "        \"\"\"\n",
    "        doc=doc.lower()\n",
    "        doc=re.sub(r'-',' ',doc)\n",
    "        doc=re.sub(r' +',' ',doc) # turn multiple spaces into a single space\n",
    "        doc=re.sub(r'[^a-z ]','',doc) # remove anything that is not a-z or space\n",
    "        words=doc.split()\n",
    "        word_vocab=[vocab.get(word,-1) for word in words]\n",
    "        words_dict=collections.Counter(word_vocab)\n",
    "        del words_dict[-1] # ignore the words outside the vocabulary\n",
    "        #wordid=words_dict.keys()\n",
    "        #wordcnt=words_dict.values()\n",
    "        return sorted(words_dict.items())\n",
    "    def e_step(self,N,k,V,alpha,beta,word_dict,conv_threshold,max_iter):\n",
    "        \"\"\"\n",
    "        Variational inference algorithm for document-specific parameters of a single doc in LDA with the equivalent class representation.\n",
    "        Arguments:\n",
    "        N: number of words\n",
    "        k: number of topics\n",
    "        V: length of vocabulary\n",
    "        alpha: corpus-level Dirichlet parameter, k-vector\n",
    "        beta: corpus-level multinomial parameter, k * V matrix\n",
    "        word_dict: word_dict from parse_doc\n",
    "        conv_threshold: threshold for convergence\n",
    "        max_iter: maximum number of iterations\n",
    "        Output:\n",
    "        A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "        First element: $\\gamma^*$, k-vector\n",
    "        Second element: the second sum in Eq(9), k*V matrix\n",
    "        \"\"\"\n",
    "        conv=False\n",
    "        wordid=list(map(lambda x:x[0],word_dict))\n",
    "        wordcnt=list(map(lambda x:x[1],word_dict))\n",
    "        phi0=np.full(shape=(len(wordid),k),fill_value=1/k) # phi_tilde \n",
    "        phi1=np.zeros(shape=(len(wordid),k))\n",
    "        gamma0=alpha+N/k\n",
    "        for it in range(max_iter):\n",
    "            for j in range(len(wordid)):\n",
    "                # the jth row of phi1 corresponds to the word labelled as wordid[j]\n",
    "                for i in range(k):\n",
    "                    #phi1[j,i]=beta[i,wordid[j]]*np.exp(psi(gamma0[i]))*wordcnt[j]\n",
    "                    phi1[j,i]=beta[i,wordid[j]]*np.exp(psi(gamma0[i]))\n",
    "                phi1[j,]=phi1[j,]/np.sum(phi1[j,])\n",
    "            gamma1=alpha+np.sum(phi1*(np.array(wordcnt).reshape((-1,1))),axis=0)\n",
    "            #gamma1=alpha+np.sum(phi1,axis=0)\n",
    "            # stop if gamma has converged\n",
    "            if np.max(np.abs((gamma0-gamma1)))<conv_threshold:\n",
    "                conv=True\n",
    "                break\n",
    "            gamma0=gamma1\n",
    "            phi0=phi1 \n",
    "        if not conv:\n",
    "            warn('Variational inference has not converged. Try more iterations.')\n",
    "        suff_stat=np.zeros(shape=(V,k))\n",
    "        suff_stat[wordid,]=phi1*(np.array(wordcnt).reshape((-1,1)))\n",
    "        return (gamma1,suff_stat.T) \n",
    "    def m_step_exp(self,M,k,V,suff_stat_list,gamma_list,alpha0,conv_threshold,max_iter):\n",
    "        \"\"\"\n",
    "        M-step in variational EM, maximizing the lower bound on log-likelihood w.r.t. alpha and beta. (Section 5.3)\n",
    "        Arguments:\n",
    "        M: number of documents in the corpus\n",
    "        k: number of topics\n",
    "        V: length of vocab\n",
    "        suff_stat_list: M-list of sufficient statistics (k * V matrices), one for each doc\n",
    "        gamma_list: M-list of gamma's (k-vectors), one for each doc\n",
    "        alpha0: initialization of alpha in Newton-Raphson\n",
    "        conv_threshold: convergence threshold in Newton-Raphson\n",
    "        max_iter: maximum number of iterations in Newton-Raphson\n",
    "        Output:\n",
    "        A 2-tuple. \n",
    "        First element: beta (k*V matrix)\n",
    "        Second element: alpha (k*1)\n",
    "        \"\"\"\n",
    "        alphalist=[alpha0]\n",
    "        ll=[]\n",
    "        ll0=conv_threshold\n",
    "        conv=False\n",
    "        # update beta\n",
    "        beta=reduce(lambda x,y: x+y, suff_stat_list)\n",
    "        beta=beta/np.sum(beta,axis=1).reshape((-1,1))\n",
    "        # update alpha (Newton-Raphson)\n",
    "        alpha0=alpha0.reshape((k,1))\n",
    "        psi_sum_gamma=np.array(list(map(lambda x: psi(np.sum(x)),gamma_list))).reshape((M,1)) # M*1 \n",
    "        psi_gamma=psi(np.array(gamma_list)) # M*k matrix\n",
    "        for it in range(max_iter):\n",
    "            a0=np.log(alpha0)\n",
    "            psi_sum_alpha=psi(np.sum(alpha0))\n",
    "            poly_sum_alpha=polygamma(1,np.sum(alpha0))\n",
    "            g=M*(psi_sum_alpha-psi(alpha0)).reshape((k,1))+np.sum(psi_gamma-psi_sum_gamma,axis=0).reshape((k,1))*alpha0.reshape((k,1)) # k*1\n",
    "            H=alpha0@alpha0.T*M*poly_sum_alpha+np.diag(g.reshape((k,))+1e-10-(alpha0**2*M*polygamma(1,alpha0)).reshape((k,)))\n",
    "            a1=a0-np.linalg.inv(H)@g\n",
    "            alpha1=np.exp(a1)\n",
    "            ll1=utilities.loglik(alpha1,gamma_list,M,k)\n",
    "            ll.append(ll1)\n",
    "            if np.abs((ll1-ll0)/ll0)<conv_threshold:\n",
    "                #print('newton finished at iteration',it)\n",
    "                conv=True\n",
    "                break\n",
    "            alpha0=alpha1\n",
    "            a0=np.log(alpha0)\n",
    "            alphalist.append(alpha1)\n",
    "            ll0=ll1\n",
    "        if not conv:\n",
    "            warn('Newton-Raphson has not converged. Try more iterations.')\n",
    "        return (beta,alpha1,ll,alphalist)\n",
    "    def variational_em_all(self,Nd,alpha0,beta0,word_dicts,vocab,M,k, conv_threshold,max_iter,niter,m_func=m_step_exp):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "        Nd: list of length of documents \n",
    "        alpha0: initialization of alpha\n",
    "        beta0: initialization of beta. DO NOT initialize with identical rows!\n",
    "        word_dicts: list of word_dict of documents, in the same order as N\n",
    "        vocab: vocabulary\n",
    "        M: number of documents\n",
    "        k: number of topics\n",
    "        \"\"\"\n",
    "        V=len(vocab)\n",
    "        for it in range(niter):\n",
    "            print(it)\n",
    "            e_estimates=list(map(lambda x,y: self.e_step(x,k,V,alpha0,beta0,y,conv_threshold=conv_threshold,max_iter=max_iter), Nd,word_dicts))\n",
    "            gamma_list=list(map(lambda x:x[0],e_estimates))\n",
    "            suff_stat_list=list(map(lambda x:x[1],e_estimates))\n",
    "            m_estimates=m_func(self,M,k,V,suff_stat_list,gamma_list,alpha0,conv_threshold=conv_threshold,max_iter=max_iter)\n",
    "            alpha1=m_estimates[1]\n",
    "            beta1=m_estimates[0]\n",
    "            if np.max(np.abs(beta1-beta0))<conv_threshold:\n",
    "                #print('vem finished at iteration',it)\n",
    "                break\n",
    "            alpha0=alpha1.reshape(k)\n",
    "            beta0=beta1\n",
    "        return (alpha0,beta0)\n",
    "    def lda(self,num_topics,num_words=None,alpha0='rand_init',beta0='rand_init',conv_threshold=1e-3,max_iter=int(1e3),niter=int(1e3)):\n",
    "        \"\"\"Fit LDA to the corpus with given number of topics. Returns the words with highest probablity in each topic.\"\"\"\n",
    "        vocab=self.make_vocab_from_docs()\n",
    "        word_dicts=list(map(lambda x: self.parse_doc(x,vocab),self.docs))\n",
    "        Nd=list(map(len,self.docs))\n",
    "        k,M,V=num_topics,len(self.docs),len(self.vocab)\n",
    "        if alpha0=='rand_init':\n",
    "            np.random.seed(1)\n",
    "            alpha0=np.exp(np.random.random(k))\n",
    "        if beta0=='rand_init':\n",
    "            np.random.seed(3)\n",
    "            str_whole=reduce(lambda x,y:x+' '+y, self.docs)\n",
    "            pd=self.parse_doc(str_whole,vocab)\n",
    "            #beta0=np.array([w[1] for w in pd]*k).reshape((k,V))\n",
    "            beta0=np.random.random((k,V))\n",
    "            beta0=beta0/np.sum(beta0,axis=1).reshape((-1,1))\n",
    "        vem=self.variational_em_all(Nd,alpha0,beta0,word_dicts,vocab,M,k, conv_threshold,max_iter,niter)\n",
    "        beta_post=vem[1]\n",
    "        topics=[dict(zip(list(vocab.keys()),beta_post[i,:])) for i in range(k)]\n",
    "        topics=[sorted(topic.items(),key=lambda x:x[1],reverse=True) for topic in topics]\n",
    "        self.topics=topics\n",
    "        if num_words:\n",
    "            return [topic[0:num_words] for topic in topics]\n",
    "        else: \n",
    "            return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "valued-vault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('b', 1.0), ('a', 0.0)], [('a', 1.0), ('b', 0.0)]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1='a '*10\n",
    "d2='b '*10\n",
    "docs=[d1,d2]*10\n",
    "a=LDA2(docs)\n",
    "a.lda(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bulgarian-transformation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('a', 0.06741573033707865),\n",
       "  ('her', 0.056179775280898875),\n",
       "  ('the', 0.056179775280898875),\n",
       "  ('and', 0.033707865168539325),\n",
       "  ('alice', 0.02247191011235955)],\n",
       " [('and', 0.057692307692307696),\n",
       "  ('a', 0.038461538461538464),\n",
       "  ('of', 0.038461538461538464),\n",
       "  ('java', 0.028846153846153848),\n",
       "  ('is', 0.028846153846153848)]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = \"Java is a language for programming that develops a software for several platforms. A compiled code or bytecode on Java application can run on most of the operating systems including Linux, Mac operating system, and Linux. Most of the syntax of Java is derived from the C++ and C languages.\"\n",
    "d2 = \"Python supports multiple programming paradigms and comes up with a large standard library, paradigms included are object-oriented, imperative, functional and procedural.\"\n",
    "d3 = \"Go is typed statically compiled language. It was created by Robert Griesemer, Ken Thompson, and Rob Pike in 2009. This language offers garbage collection, concurrency of CSP-style, memory safety, and structural typing.\"\n",
    "d4 = \"A young girl when she first visited magical Underland, Alice Kingsleigh (Mia Wasikowska) is now a teenager with no memory of the place -- except in her dreams.\"\n",
    "d5 = \"Her life takes a turn for the unexpected when, at a garden party for her fiance and herself, she spots a certain white rabbit and tumbles down a hole after him. Reunited with her friends the Mad Hatter (Johnny Depp), the Cheshire Cat and others, Alice learns it is her destiny to end the Red Queen's (Helena Bonham Carter) reign of terror.\"\n",
    "docs=[d1,d2,d3,d4,d5]\n",
    "b=LDA2(docs)\n",
    "b.lda(2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-affiliate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
