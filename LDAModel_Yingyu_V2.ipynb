{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rapid-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.special import psi  # gamma function utils\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imposed-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils and Helper Class\n",
    "\n",
    "def tf(docs):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the document-term matrix and id2word mapping\n",
    "    \"\"\"\n",
    "    # Clean up the text\n",
    "    docsc_clean = {}\n",
    "    total_term = []\n",
    "    for key, val in enumerate(docs):\n",
    "        val_clean = re.findall(r'[a-z]+', val.lower())\n",
    "        docsc_clean[f'd{key}'] = val_clean\n",
    "        total_term += val_clean\n",
    "\n",
    "    total_term_unique = sorted(set(total_term))\n",
    "    id2word = {idx: word for  idx, word in enumerate(total_term_unique)}\n",
    "\n",
    "    # Count the number of occurrences of term i in document j\n",
    "    for key, val in docsc_clean.items():\n",
    "        word_dir = dict.fromkeys(total_term_unique, 0)\n",
    "        for word in val:\n",
    "            word_dir[word] += 1\n",
    "        docsc_clean[key] = word_dir\n",
    "\n",
    "    tf_df = pd.DataFrame.from_dict(docsc_clean, orient='index')\n",
    "\n",
    "    return tf_df, id2word\n",
    "\n",
    "def dirichlet_expectation(sstats):\n",
    "    if len(sstats.shape) == 1:\n",
    "        return psi(sstats) - psi(np.sum(sstats))\n",
    "    else:\n",
    "        return psi(sstats) - psi(np.sum(sstats, 1))[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "class LdaState:\n",
    "    def __init__(self, eta, shape, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : numpy.ndarray\n",
    "            The prior probabilities assigned to each term.\n",
    "        shape : tuple of (int, int)\n",
    "            Shape of the sufficient statistics: (number of topics to be found, number of terms in the vocabulary).\n",
    "        dtype : type\n",
    "            Overrides the numpy array default types.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta.astype(dtype, copy=False)\n",
    "        self.sstats = np.zeros(shape, dtype=dtype)\n",
    "        self.numdocs = 0\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def get_lambda(self):\n",
    "        \"\"\"Get the parameters of the posterior over the topics, also referred to as \"the topics\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Parameters of the posterior probability over topics.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.eta + self.sstats\n",
    "\n",
    "    def get_Elogbeta(self):\n",
    "        \"\"\"Get the log (posterior) probabilities for each topic.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Posterior probabilities for each topic.\n",
    "        \"\"\"\n",
    "        return dirichlet_expectation(self.get_lambda())\n",
    "\n",
    "    def blend(self, rhot, other, targetsize=None):\n",
    "        \"\"\"Merge the current state with another one using a weighted average for the sufficient statistics.\n",
    "\n",
    "        The number of documents is stretched in both state objects, so that they are of comparable magnitude.\n",
    "        This procedure corresponds to the stochastic gradient update from\n",
    "        `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\"\n",
    "        <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_, see equations (5) and (9).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rhot : float\n",
    "            Weight of the `other` state in the computed average. A value of 0.0 means that `other`\n",
    "            is completely ignored. A value of 1.0 means `self` is completely ignored.\n",
    "        other : :class:`~gensim.models.ldamodel.LdaState`\n",
    "            The state object with which the current one will be merged.\n",
    "        targetsize : int, optional\n",
    "            The number of documents to stretch both states to.\n",
    "\n",
    "        \"\"\"\n",
    "        assert other is not None\n",
    "        if targetsize is None:\n",
    "            targetsize = self.numdocs\n",
    "\n",
    "        # stretch the current model's expected n*phi counts to target size\n",
    "        if self.numdocs == 0 or targetsize == self.numdocs:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0 * targetsize / self.numdocs\n",
    "        self.sstats *= (1.0 - rhot) * scale\n",
    "\n",
    "        # stretch the incoming n*phi counts to target size\n",
    "        if other.numdocs == 0 or targetsize == other.numdocs:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0 * targetsize / other.numdocs\n",
    "        self.sstats += rhot * scale * other.sstats\n",
    "        self.numdocs = targetsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "plastic-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lda_func(corpus, num_topics, id2word, random_state=10,  passes=1, num_words=10,\n",
    "                iterations=50, gamma_threshold=0.001, dtype=np.float32,  chunksize=100, topics_only=True, verbose=False):\n",
    "    num_terms = len(id2word)\n",
    "\n",
    "    alpha = np.array( [1.0 / num_topics for i in range(num_topics)], dtype=dtype)\n",
    "\n",
    "    eta = np.array( [1.0 / num_topics for i in range(num_terms)], dtype=dtype)\n",
    "\n",
    "    rand  = np.random.RandomState(random_state)\n",
    "\n",
    "    model_states = LdaState(eta, (num_topics, num_terms), dtype=dtype)\n",
    "    model_states.sstats = rand.gamma(100., 1. / 100., (num_topics, num_terms))\n",
    "\n",
    "    expElogbeta = np.exp(dirichlet_expectation(model_states.sstats))\n",
    "\n",
    "\n",
    "    # Update\n",
    "    lencorpus = len(corpus)\n",
    "    chunksize = min(lencorpus, chunksize)\n",
    "    model_states.numdocs += lencorpus\n",
    "    num_updates = 0\n",
    "\n",
    "    for pass_ in range(passes):\n",
    "        all_chunks = chunks(corpus, chunksize)\n",
    "\n",
    "        for chunk_no, chunk in enumerate(all_chunks):\n",
    "            other = LdaState(eta, (num_topics, num_terms), dtype=dtype)\n",
    "            # Do estep\n",
    "            if len(chunk) > 1:\n",
    "                if verbose:\n",
    "                    print(f'performing inference on a chunk of {len(chunk) } documents')\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "            # Initialize the variational distribution q(theta|gamma) for the chunk\n",
    "            gamma = rand.gamma(100., 1. / 100., (len(chunk), num_topics)).astype(dtype, copy=False)\n",
    "            tmpElogtheta = dirichlet_expectation(gamma)\n",
    "            tmpexpElogtheta = np.exp(tmpElogtheta)\n",
    "            sstats = np.zeros_like(expElogbeta, dtype=dtype)\n",
    "            converged = 0\n",
    "\n",
    "            # Now, for each document d update that document's gamma and phi\n",
    "            epsilon = 1e-7\n",
    "\n",
    "            for d, doc in enumerate(chunk):\n",
    "                ids = [idx for idx, _ in doc]\n",
    "                cts = np.fromiter((cnt for _, cnt in doc), dtype=dtype, count=len(doc))\n",
    "                gammad = gamma[d, :]\n",
    "                Elogthetad = tmpElogtheta[d, :]\n",
    "                expElogthetad = tmpexpElogtheta[d, :]\n",
    "                expElogbetad = expElogbeta[:, ids]\n",
    "\n",
    "                # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\n",
    "                # phinorm is the normalizer.\n",
    "                phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
    "\n",
    "                for _ in range(iterations):\n",
    "                    lastgamma = gammad\n",
    "                    # We represent phi implicitly to save memory and time.\n",
    "                    # Substituting the value of the optimal phi back into\n",
    "                    # the update for gamma gives this update. Cf. Lee&Seung 2001.\n",
    "                    gammad = alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
    "                    Elogthetad = dirichlet_expectation(gammad)\n",
    "                    expElogthetad = np.exp(Elogthetad)\n",
    "                    phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
    "                    # If gamma hasn't changed much, we're done.\n",
    "                    meanchange = np.mean(np.abs(gammad - lastgamma))\n",
    "                    if meanchange < gamma_threshold:\n",
    "                        converged += 1\n",
    "                        break\n",
    "                gamma[d, :] = gammad\n",
    "                sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)\n",
    "\n",
    "            if len(chunk) > 1:\n",
    "                if verbose:\n",
    "                    print(f\"{converged}/{len(chunk)} documents converged within {iterations} iterations\")\n",
    "\n",
    "            sstats *= expElogbeta\n",
    "\n",
    "            other.sstats += sstats\n",
    "            other.numdocs += gamma.shape[0]\n",
    "\n",
    "            # Do mstep\n",
    "            if verbose:\n",
    "                print('Update topics')\n",
    "            previous_Elogbeta = model_states.get_Elogbeta()\n",
    "            rho = pow(1 + pass_ + (num_updates / chunksize), -0.5)\n",
    "            model_states.blend(rho, other)\n",
    "\n",
    "            current_Elogbeta = model_states.get_Elogbeta()\n",
    "            #Propagate the states topic probabilities to the inner object's attribute.\n",
    "            expElogbeta = np.exp(current_Elogbeta)\n",
    "\n",
    "            diff = np.mean(np.abs(previous_Elogbeta.ravel() - current_Elogbeta.ravel()))\n",
    "            if verbose:\n",
    "                print(f\"topic diff {diff}\")\n",
    "            num_updates += other.numdocs\n",
    "\n",
    "    shown = []\n",
    "    topic = model_states.get_lambda()\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        topic_ = topic[i]\n",
    "        topic_ = topic_ / topic_.sum()  # normalize to probability distribution\n",
    "        bestn = topic_.argsort()[-num_words:][::-1]\n",
    "\n",
    "        topic_ = [(id2word[id], topic_[id]) for id in bestn]\n",
    "        topic_ = ' + '.join('%.3f*\"%s\"' % (v, k) for k, v in topic_)\n",
    "        shown.append((i, topic_))\n",
    "\n",
    "    if topics_only:\n",
    "        return shown\n",
    "    else:\n",
    "        return shown,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-company",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-salmon",
   "metadata": {},
   "source": [
    "## For Carol: Small test data (d1 to d5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "royal-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for analysis\n",
    "d1 = \"Java is a language for programming that develops a software for several platforms. A compiled code or bytecode on Java application can run on most of the operating systems including Linux, Mac operating system, and Linux. Most of the syntax of Java is derived from the C++ and C languages.\"\n",
    "d2 = \"Python supports multiple programming paradigms and comes up with a large standard library, paradigms included are object-oriented, imperative, functional and procedural.\"\n",
    "d3 = \"Go is typed statically compiled language. It was created by Robert Griesemer, Ken Thompson, and Rob Pike in 2009. This language offers garbage collection, concurrency of CSP-style, memory safety, and structural typing.\"\n",
    "d4 = \"A young girl when she first visited magical Underland, Alice Kingsleigh (Mia Wasikowska) is now a teenager with no memory of the place -- except in her dreams.\"\n",
    "d5 = \"Her life takes a turn for the unexpected when, at a garden party for her fiance and herself, she spots a certain white rabbit and tumbles down a hole after him. Reunited with her friends the Mad Hatter (Johnny Depp), the Cheshire Cat and others, Alice learns it is her destiny to end the Red Queen's (Helena Bonham Carter) reign of terror.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wired-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.038*\"and\" + 0.036*\"a\" + 0.026*\"the\" + 0.018*\"of\" + 0.017*\"her\" + '\n",
      "  '0.015*\"is\" + 0.013*\"for\" + 0.013*\"language\" + 0.013*\"paradigms\" + '\n",
      "  '0.011*\"with\"'),\n",
      " (1,\n",
      "  '0.030*\"a\" + 0.029*\"the\" + 0.025*\"of\" + 0.023*\"and\" + 0.022*\"is\" + '\n",
      "  '0.019*\"her\" + 0.017*\"for\" + 0.015*\"java\" + 0.013*\"with\" + 0.012*\"when\"')]\n"
     ]
    }
   ],
   "source": [
    "# Using slow version tf_df\n",
    "tf_df, id2word = tf([d1, d2, d3, d4, d5])\n",
    "\n",
    "lil = []\n",
    "for row in tf_df.values:\n",
    "    lil_sub = []\n",
    "    for idx, item in enumerate(row):\n",
    "        if item:\n",
    "            lil_sub.append((idx, item))\n",
    "    lil.append(lil_sub)\n",
    "    \n",
    "pprint(my_lda_func(corpus=lil, num_topics=2, id2word=id2word, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-superior",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "secure-timeline",
   "metadata": {},
   "source": [
    "## For Melody: Real world data (from Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "baking-appliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Real world sample data\n",
    "raw_tweets = pd.read_csv('clean_tweets.csv')\n",
    "\n",
    "tweets_list = raw_tweets.Tweets.values.tolist()\n",
    "\n",
    "# Turn the list of string into a list of tokens\n",
    "clean_tweets = [t.split(',') for t in tweets_list]\n",
    "\n",
    "len(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aggressive-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(clean_tweets)\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in clean_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "intimate-pointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"portfolio\" + 0.015*\"employment\" + 0.011*\"nursing\" + 0.010*\"repair\" + '\n",
      "  '0.009*\"prevention\" + 0.009*\"consultation\" + 0.009*\"command\" + '\n",
      "  '0.008*\"terminal\" + 0.008*\"briefly\" + 0.008*\"shall\"'),\n",
      " (1,\n",
      "  '0.032*\"trump\" + 0.012*\"say\" + 0.010*\"vote\" + 0.007*\"country\" + 0.007*\"ban\" '\n",
      "  '+ 0.006*\"people\" + 0.006*\"order\" + 0.005*\"need\" + 0.005*\"state\" + '\n",
      "  '0.005*\"right\"'),\n",
      " (2,\n",
      "  '0.021*\"thank\" + 0.018*\"great\" + 0.016*\"good\" + 0.012*\"look\" + 0.010*\"year\" '\n",
      "  '+ 0.009*\"today\" + 0.009*\"day\" + 0.009*\"time\" + 0.009*\"love\" + 0.009*\"new\"'),\n",
      " (3,\n",
      "  '0.013*\"new\" + 0.010*\"how\" + 0.006*\"work\" + 0.006*\"learn\" + 0.006*\"great\" + '\n",
      "  '0.006*\"change\" + 0.006*\"need\" + 0.005*\"business\" + 0.005*\"help\" + '\n",
      "  '0.005*\"story\"'),\n",
      " (4,\n",
      "  '0.007*\"cove\" + 0.000*\"peggys\" + 0.000*\"lagoon\" + 0.000*\"killing\" + '\n",
      "  '0.000*\"fate\" + 0.000*\"infamous\" + 0.000*\"creek\" + 0.000*\"await\" + '\n",
      "  '0.000*\"roaster\" + 0.000*\"oakridge\"'),\n",
      " (5,\n",
      "  '0.153*\"more\" + 0.052*\"today\" + 0.018*\"cancer\" + 0.016*\"pisce\" + '\n",
      "  '0.013*\"capricorn\" + 0.012*\"aquarius\" + 0.010*\"arie\" + 0.007*\"feel\" + '\n",
      "  '0.006*\"day\" + 0.006*\"gemini\"'),\n",
      " (6,\n",
      "  '0.017*\"love\" + 0.016*\"day\" + 0.016*\"go\" + 0.011*\"good\" + 0.010*\"time\" + '\n",
      "  '0.009*\"feel\" + 0.009*\"know\" + 0.008*\"today\" + 0.008*\"happy\" + 0.008*\"work\"'),\n",
      " (7,\n",
      "  '0.111*\"video\" + 0.092*\"like\" + 0.077*\"follow\" + 0.066*\"check\" + '\n",
      "  '0.042*\"thank\" + 0.035*\"late\" + 0.033*\"people\" + 0.029*\"automatically\" + '\n",
      "  '0.028*\"new\" + 0.020*\"unfollowed\"'),\n",
      " (8,\n",
      "  '0.067*\"enter\" + 0.013*\"prize\" + 0.013*\"medal\" + 0.011*\"brick\" + '\n",
      "  '0.010*\"lunar\" + 0.008*\"7th\" + 0.007*\"win\" + 0.007*\"sweepstake\" + '\n",
      "  '0.006*\"pack\" + 0.004*\"spa\"'),\n",
      " (9,\n",
      "  '0.016*\"think\" + 0.012*\"know\" + 0.012*\"people\" + 0.010*\"good\" + 0.008*\"go\" + '\n",
      "  '0.008*\"thing\" + 0.007*\"time\" + 0.007*\"need\" + 0.006*\"say\" + 0.006*\"right\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(my_lda_func(corpus=corpus, num_topics=10, id2word=id2word, num_words=10, chunksize=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-central",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-penalty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-packet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "united-benjamin",
   "metadata": {},
   "source": [
    "## Compare with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "multiple-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "collected-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   num_topics=10, \n",
    "                   random_state=10,\n",
    "                   chunksize=100,\n",
    "#                    alpha='auto',\n",
    "#                    per_word_topics=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "wired-arrest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.021*\"portfolio\" + 0.013*\"employment\" + 0.010*\"nursing\" + 0.009*\"repair\" + '\n",
      "  '0.008*\"prevention\" + 0.008*\"command\" + 0.008*\"consultation\" + '\n",
      "  '0.007*\"terminal\" + 0.007*\"briefly\" + 0.007*\"shall\"'),\n",
      " (1,\n",
      "  '0.029*\"trump\" + 0.011*\"say\" + 0.009*\"vote\" + 0.007*\"country\" + 0.006*\"ban\" '\n",
      "  '+ 0.006*\"people\" + 0.006*\"order\" + 0.006*\"need\" + 0.005*\"state\" + '\n",
      "  '0.005*\"right\"'),\n",
      " (2,\n",
      "  '0.021*\"thank\" + 0.018*\"great\" + 0.016*\"good\" + 0.012*\"look\" + 0.010*\"year\" '\n",
      "  '+ 0.009*\"today\" + 0.009*\"time\" + 0.009*\"day\" + 0.009*\"love\" + 0.008*\"new\"'),\n",
      " (3,\n",
      "  '0.013*\"new\" + 0.010*\"how\" + 0.007*\"work\" + 0.006*\"learn\" + 0.006*\"great\" + '\n",
      "  '0.006*\"change\" + 0.006*\"need\" + 0.005*\"business\" + 0.005*\"help\" + '\n",
      "  '0.005*\"social\"'),\n",
      " (4,\n",
      "  '0.007*\"cove\" + 0.000*\"killing\" + 0.000*\"peggys\" + 0.000*\"lagoon\" + '\n",
      "  '0.000*\"infamous\" + 0.000*\"fate\" + 0.000*\"await\" + 0.000*\"creek\" + '\n",
      "  '0.000*\"maiden\" + 0.000*\"roaster\"'),\n",
      " (5,\n",
      "  '0.157*\"more\" + 0.053*\"today\" + 0.018*\"cancer\" + 0.017*\"pisce\" + '\n",
      "  '0.013*\"capricorn\" + 0.012*\"aquarius\" + 0.011*\"arie\" + 0.008*\"feel\" + '\n",
      "  '0.006*\"day\" + 0.006*\"gemini\"'),\n",
      " (6,\n",
      "  '0.016*\"love\" + 0.016*\"day\" + 0.016*\"go\" + 0.011*\"good\" + 0.010*\"time\" + '\n",
      "  '0.009*\"know\" + 0.008*\"feel\" + 0.008*\"today\" + 0.008*\"work\" + 0.008*\"happy\"'),\n",
      " (7,\n",
      "  '0.112*\"video\" + 0.093*\"like\" + 0.078*\"follow\" + 0.067*\"check\" + '\n",
      "  '0.042*\"thank\" + 0.036*\"late\" + 0.033*\"people\" + 0.030*\"automatically\" + '\n",
      "  '0.028*\"new\" + 0.021*\"unfollowed\"'),\n",
      " (8,\n",
      "  '0.069*\"enter\" + 0.016*\"prize\" + 0.011*\"brick\" + 0.010*\"offense\" + '\n",
      "  '0.010*\"lunar\" + 0.009*\"win\" + 0.008*\"instantly\" + 0.007*\"pack\" + '\n",
      "  '0.007*\"sweepstake\" + 0.004*\"gadget\"'),\n",
      " (9,\n",
      "  '0.016*\"think\" + 0.012*\"people\" + 0.012*\"know\" + 0.010*\"good\" + 0.009*\"go\" + '\n",
      "  '0.008*\"thing\" + 0.007*\"time\" + 0.007*\"need\" + 0.006*\"say\" + 0.006*\"right\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-course",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
