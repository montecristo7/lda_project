{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accessory-software",
   "metadata": {},
   "source": [
    "# The Variational EM algorithm of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "familiar-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import random\n",
    "from scipy.special import gammaln, psi, polygamma\n",
    "from functools import reduce\n",
    "from warnings import warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "written-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc(doc,vocab):\n",
    "    \"\"\"\n",
    "    Parse a single document. \n",
    "    Arguments:\n",
    "    doc: document string\n",
    "    vocab: a dictionary that maps words to integers\n",
    "    Output:\n",
    "    A dictionary, where the keys are words appeared in the doc, labeled with the integers in the vocab dictionary (the set of $\\tilde{w_n}$), \n",
    "        and the values are counts of the words.\n",
    "    The words that are not in vocab will be ignored.\n",
    "    \"\"\"\n",
    "    doc=doc.lower()\n",
    "    doc=re.sub(r'-',' ',doc)\n",
    "    doc=re.sub(r' +',' ',doc) # turn multiple spaces into a single space\n",
    "    doc=re.sub(r'[^a-z ]','',doc) # remove anything that is not a-z or space\n",
    "    words=doc.split()\n",
    "    word_vocab=[vocab.get(word,-1) for word in words]\n",
    "    words_dict=collections.Counter(word_vocab)\n",
    "    del words_dict[-1] # ignore the words outside the vocabulary\n",
    "    #wordid=words_dict.keys()\n",
    "    #wordcnt=words_dict.values()\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "drawn-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1, 2: 1, 0: 2, 4: 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of parsing a document\n",
    "doc='two-three one, Four*, five, one ^#%$@#@#'\n",
    "vocab=dict(zip(('one','two','three','sth','four','else'),range(6)))\n",
    "parsed_doc=parse_doc(doc,vocab)\n",
    "parsed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-studio",
   "metadata": {},
   "source": [
    "## The equivalent class of words\n",
    "Suppose we have a single document. Let \n",
    "\n",
    "$$\\tilde{w_n}=\\text{j such that }w_n^j=1$$\n",
    "To carry out the inference, first note that for fixed $i,j$, $\\{\\phi_{ni}: \\tilde{w_n}=j\\}$ will all be the same. This suggests us to define the following quantity:\n",
    "\n",
    "$$\\tilde{\\phi_{ji}}=\\phi_{ni}, \\text{ where }\\tilde{w_n}=j$$\n",
    "We only need to update $\\tilde{\\phi}$ in the variational inference (E-step). For fixed $i$, the length of $\\tilde{\\phi}_{:,i}$ is the cardinality of $\\{\\tilde{w_n}: n=1,\\cdots, N\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sorted-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_inference(N,k,V,alpha,beta,w,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Variational inference algorithm for document-specific parameters of a single doc in LDA (figure 6 in the paper)\n",
    "    Arguments:\n",
    "    N: number of words in the document\n",
    "    k: number of topics\n",
    "    V: length of vocabulary\n",
    "    alpha: corpus-level Dirichlet parameter, k-vector\n",
    "    beta: corpus-level multinomial parameter, k * V matrix\n",
    "    w: word id obtained from parsing the document\n",
    "    conv_threshold: threshold for convergence\n",
    "    max_iter: maximum number of iterations\n",
    "    Output:\n",
    "    A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "    First element: $\\gamma^*$, k-vector\n",
    "    Second element: the second sum in Eq(9), k*V matrix\n",
    "    \"\"\"\n",
    "    phi0=np.full(shape=(N,k),fill_value=1/k) \n",
    "    phi1=phi0\n",
    "    gamma0=alpha+N/k\n",
    "    for it in range(max_iter):\n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                phi1[n,i]=beta[i,w[n]]*np.exp(psi(gamma0[i]))\n",
    "            phi1[n,]=phi1[n,]/np.sum(phi1[n,])\n",
    "        gamma1=alpha+np.sum(phi1,axis=0)\n",
    "        # stop if gamma has converged\n",
    "        if np.mean(np.abs(gamma0-gamma1))<conv_threshold:\n",
    "            break\n",
    "        gamma0=gamma1\n",
    "        phi0=phi1 \n",
    "    suff_stat=np.zeros(shape=(V,k))\n",
    "    for n in range(N):\n",
    "        suff_stat[w[n],]=suff_stat[w[n],]+phi1[n,]\n",
    "    return (gamma1,suff_stat.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "built-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(N,k,V,alpha,beta,word_dict,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Variational inference algorithm for document-specific parameters of a single doc in LDA with the equivalent class representation.\n",
    "    Arguments:\n",
    "    N: number of words\n",
    "    k: number of topics\n",
    "    V: length of vocabulary\n",
    "    alpha: corpus-level Dirichlet parameter, k-vector\n",
    "    beta: corpus-level multinomial parameter, k * V matrix\n",
    "    word_dict: word_dict from parse_doc\n",
    "    conv_threshold: threshold for convergence\n",
    "    max_iter: maximum number of iterations\n",
    "    Output:\n",
    "    A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "    First element: $\\gamma^*$, k-vector\n",
    "    Second element: the second sum in Eq(9), k*V matrix\n",
    "    \"\"\"\n",
    "    conv=False\n",
    "    wordid=list(word_dict.keys())\n",
    "    wordcnt=list(word_dict.values())\n",
    "    phi0=np.full(shape=(len(wordid),k),fill_value=1/k) # phi_tilde \n",
    "    phi1=phi0\n",
    "    gamma0=alpha+N/k\n",
    "    for it in range(max_iter):\n",
    "        for j in range(len(wordid)):\n",
    "            for i in range(k):\n",
    "                phi1[j,i]=beta[i,wordid[j]]*np.exp(psi(gamma0[i]))*wordcnt[j]\n",
    "            phi1[j,]=phi1[j,]/np.sum(phi1[j,])\n",
    "        gamma1=alpha+np.sum(phi1*np.array(wordcnt).reshape((-1,1)),axis=0)\n",
    "        # stop if gamma has converged\n",
    "        if np.mean(np.abs(gamma0-gamma1))<conv_threshold:\n",
    "            conv=True\n",
    "            break\n",
    "        gamma0=gamma1\n",
    "        phi0=phi1 \n",
    "    if not conv:\n",
    "        warn('Variational inference has not converged. Try more iterations.')\n",
    "    suff_stat=np.zeros(shape=(V,k))\n",
    "    suff_stat[wordid,]=phi1*np.array(wordcnt).reshape((-1,1))\n",
    "    return (gamma1,suff_stat.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "abstract-organic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 4.4408921e-16, 0.0000000e+00])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the two functions produce close results\n",
    "random.seed(1)\n",
    "N=5\n",
    "k=3\n",
    "V=6\n",
    "alpha=np.array([1,2,3])\n",
    "beta=np.random.randint(low=1,high=10,size=(3,6))\n",
    "beta=beta/np.sum(beta,axis=1).reshape((-1,1))\n",
    "word_dict=parsed_doc\n",
    "w=[1,2,0,4,0]\n",
    "res1=variational_inference(N,k,V,alpha,beta,w,conv_threshold=1e-5)\n",
    "res2=e_step(N,k,V,alpha,beta,word_dict,conv_threshold=1e-5)\n",
    "res1[0]-res2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "guilty-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 5.55111512e-17, 5.55111512e-17, 0.00000000e+00,\n",
       "        1.38777878e-17, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[1]-res2[1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "isolated-pierre",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "emerging-processing",
   "metadata": {},
   "source": [
    "## The M-step\n",
    "\n",
    "The parameters $\\beta$ and $\\alpha$ are updated in the M step. \n",
    "\n",
    "$\\beta$ is updated with Eq(9). \n",
    "\n",
    "$\\alpha$ is updated with Newton-Raphson:\n",
    "$$\\alpha_{new}=\\alpha_{old}-H(\\alpha_{old})^{-1}g(\\alpha_{old}),$$\n",
    "where $H(\\alpha)=(\\frac{\\partial^2\\mathcal{L}}{\\partial\\alpha_i\\partial\\alpha_j})_{k\\times k}$ is the Hessian matrix and $g(\\alpha)=(\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i})_{i=1}^k$ is the gradient. \n",
    "\n",
    "A.4.2 shows that \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i}=M \\left(\\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)-\\Psi(\\alpha_i)\\right) + \\sum_{d=1}^M \\left(\\Psi(\\gamma_{di})-\\Psi(\\sum_{j=1}^k\\gamma_{dj}) \\right),$$\n",
    "$$\\frac{\\partial^2\\mathcal{L}}{\\partial\\alpha_i\\partial\\alpha_j}=\\delta(i,j)M\\Psi'(\\alpha_i)-\\Psi'\\left(\\sum_{j=1}^k\\alpha_j\\right),$$\n",
    "i.e.,\n",
    "$$H(\\alpha)=diag(h)+1z1^T,$$\n",
    "where $$z=-\\Psi'\\left(\\sum_{j=1}^k\\alpha_j\\right),h=M\\Psi'(\\alpha)$$\n",
    "By A.2, we have \n",
    "$$(H^{-1}g)_i=\\frac{g_i-c}{h_i},$$\n",
    "where \n",
    "$$c=\\frac{\\sum_{j=1}^k g_j/h_j}{1/z+\\sum_{j=1}^k h_j^{-1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "neural-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    M-step in variational EM, maximizing the lower bound on log-likelihood w.r.t. alpha and beta. (Section 5.3)\n",
    "    Arguments:\n",
    "    M: number of documents in the corpus\n",
    "    k: number of topics\n",
    "    V: length of vocab\n",
    "    suff_stat_list: M-list of sufficient statistics (k * V matrices), one for each doc\n",
    "    gamma_list: M-list of gamma's (k-vectors), one for each doc\n",
    "    conv_threshold: convergence threshold in Newton-Raphson\n",
    "    max_iter: maximum number of iterations in Newton-Raphson\n",
    "    Output:\n",
    "    A 2-tuple. \n",
    "    First element: beta (k*V matrix)\n",
    "    Second element: alpha (k*1)\n",
    "    \"\"\"\n",
    "    conv=False\n",
    "    # update beta\n",
    "    beta=reduce(lambda x,y: x+y, suff_stat_list)\n",
    "    beta=beta/np.sum(beta,axis=1).reshape((-1,1))\n",
    "    # update alpha (Newton-Raphson)\n",
    "    alpha0=np.full((k,1),fill_value=1/k)\n",
    "    for it in range(max_iter):\n",
    "        psi_sum_alpha=psi(np.sum(alpha0))\n",
    "        psi_sum_gamma=np.array(list(map(lambda x: psi(np.sum(x)),gamma_list))).reshape((M,1)) # M*1 \n",
    "        psi_gamma=psi(np.array(gamma_list)) # M*k matrix\n",
    "        g=M*(psi_sum_alpha-psi(alpha0)).reshape((k,1))- np.sum(psi_gamma-psi_sum_gamma,axis=0).reshape((k,1)) # k*1 \n",
    "        h=M*polygamma(1,alpha0)\n",
    "        z=-polygamma(1,np.sum(alpha0))\n",
    "        c=np.sum(g/h.reshape((k,1)))/(1/z+np.sum(1/h))\n",
    "        invHg=(g-c)/h.reshape((k,1))\n",
    "        alpha1=alpha0-invHg+1e-10 # 1e-10 is added for numerical stability\n",
    "        if np.max(np.abs(alpha1-alpha0))<conv_threshold:\n",
    "            print('finished at iteration',it)\n",
    "            conv=True\n",
    "            break\n",
    "        alpha0=alpha1\n",
    "    if not conv:\n",
    "        warn('Newton-Raphson has not converged. Try more iterations.')\n",
    "    return (beta,alpha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "becoming-arkansas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished at iteration 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:39: UserWarning: Newton-Raphson has not converged. Try more iterations.\n"
     ]
    }
   ],
   "source": [
    "# an example\n",
    "M=10\n",
    "suff_stat_list=[res2[1]]*M\n",
    "gamma_list=[res2[0]]*M\n",
    "res3=m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(100))\n",
    "res4=m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
