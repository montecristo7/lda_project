{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "retained-guyana",
   "metadata": {},
   "source": [
    "# The Variational EM algorithm of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intended-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import random\n",
    "from scipy.special import psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "anonymous-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_doc(doc,vocab):\n",
    "    \"\"\"\n",
    "    Parse a single document. \n",
    "    Arguments:\n",
    "    doc: document string\n",
    "    vocab: a dictionary that maps words to integers\n",
    "    Output:\n",
    "    A dictionary, where the keys are words appeared in the doc, labeled with the integers in the vocab dictionary (the set of $\\tilde{w_n}$), \n",
    "        and the values are counts of the words.\n",
    "    The words that are not in vocab will be ignored.\n",
    "    \"\"\"\n",
    "    doc=doc.lower()\n",
    "    doc=re.sub(r'-',' ',doc)\n",
    "    doc=re.sub(r' +',' ',doc) # turn multiple spaces into a single space\n",
    "    doc=re.sub(r'[^a-z ]','',doc) # remove anything that is not a-z or space\n",
    "    words=doc.split()\n",
    "    word_vocab=[vocab.get(word,-1) for word in words]\n",
    "    words_dict=collections.Counter(word_vocab)\n",
    "    del words_dict[-1] # ignore the words outside the vocabulary\n",
    "    #wordid=words_dict.keys()\n",
    "    #wordcnt=words_dict.values()\n",
    "    return words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "forced-scroll",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 1, 2: 1, 0: 2, 3: 1})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of parsing a document\n",
    "doc='two-three one, Four*, five, one ^#%$@#@#'\n",
    "vocab=dict(zip(('one','two','three','four','sth','else'),range(6)))\n",
    "parsed_doc=parse_doc(doc,vocab)\n",
    "parsed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-pilot",
   "metadata": {},
   "source": [
    "## The equivalent class of words\n",
    "Suppose we have a single document. Let \n",
    "\n",
    "$$\\tilde{w_n}=\\text{j such that }w_n^j=1$$\n",
    "To carry out the inference, first note that for fixed $i,j$, $\\{\\phi_{ni}: \\tilde{w_n}=j\\}$ will all be the same. This suggests us to define the following quantity:\n",
    "\n",
    "$$\\tilde{\\phi_{ji}}=\\phi_{ni}, \\text{ where }\\tilde{w_n}=j$$\n",
    "We only need to update $\\tilde{\\phi}$ in the variational inference (E-step). For fixed $i$, the length of $\\tilde{\\phi}_{:,i}$ is the cardinality of $\\{\\tilde{w_n}: n=1,\\cdots, N\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "prompt-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_inference(N,k,V,alpha,beta,w,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Variational inference algorithm for document-specific parameters of a single doc in LDA (figure 6 in the paper)\n",
    "    Arguments:\n",
    "    N: number of words in the document\n",
    "    k: number of topics\n",
    "    V: length of vocabulary\n",
    "    alpha: corpus-level Dirichlet parameter, k-vector\n",
    "    beta: corpus-level multinomial parameter, k * V matrix\n",
    "    w: word id obtained from parsing the document\n",
    "    conv_threshold: threshold for convergence\n",
    "    max_iter: maximum number of iterations\n",
    "    Output:\n",
    "    A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "    First element: $\\gamma^*$, k-vector\n",
    "    Second element: the second sum in Eq(9)\n",
    "    \"\"\"\n",
    "    phi0=np.full(shape=(N,k),fill_value=1/k) \n",
    "    phi1=phi0\n",
    "    gamma0=alpha+N/k\n",
    "    for it in range(max_iter):\n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                phi1[n,i]=beta[i,w[n]]*np.exp(psi(gamma0[i]))\n",
    "            phi1[n,]=phi1[n,]/np.sum(phi1[n,])\n",
    "        gamma1=alpha+np.sum(phi1,axis=0)\n",
    "        # stop if gamma has converged\n",
    "        if np.mean(np.abs(gamma0-gamma1))<conv_threshold:\n",
    "            break\n",
    "        gamma0=gamma1\n",
    "        phi0=phi1 \n",
    "    suff_stat=np.zeros(shape=(V,k))\n",
    "    for n in range(N):\n",
    "        suff_stat[w[n],]=suff_stat[w[n],]+phi1[n,]\n",
    "    return (gamma1,suff_stat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "catholic-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(N,k,V,alpha,beta,word_dict,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Variational inference algorithm for document-specific parameters of a single doc in LDA with the equivalent class representation.\n",
    "    Arguments:\n",
    "    N: number of words\n",
    "    k: number of topics\n",
    "    V: length of vocabulary\n",
    "    alpha: corpus-level Dirichlet parameter, k-vector\n",
    "    beta: corpus-level multinomial parameter, k * V matrix\n",
    "    word_dict: word_dict from parse_doc\n",
    "    conv_threshold: threshold for convergence\n",
    "    max_iter: maximum number of iterations\n",
    "    Output:\n",
    "    A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "    First element: $\\gamma^*$, k-vector\n",
    "    Second element: the second sum in Eq(9)\n",
    "    \"\"\"\n",
    "    wordid=list(word_dict.keys())\n",
    "    wordcnt=list(word_dict.values())\n",
    "    phi0=np.full(shape=(len(wordid),k),fill_value=1/k) # phi_tilde \n",
    "    phi1=phi0\n",
    "    gamma0=alpha+N/k\n",
    "    for it in range(max_iter):\n",
    "        for j in range(len(wordid)):\n",
    "            for i in range(k):\n",
    "                phi1[j,i]=beta[i,wordid[j]]*np.exp(psi(gamma0[i]))*wordcnt[j]\n",
    "            phi1[j,]=phi1[j,]/np.sum(phi1[j,])\n",
    "        gamma1=alpha+np.sum(phi1*np.array(wordcnt).reshape((-1,1)),axis=0)\n",
    "        # stop if gamma has converged\n",
    "        if np.mean(np.abs(gamma0-gamma1))<conv_threshold:\n",
    "            break\n",
    "        gamma0=gamma1\n",
    "        phi0=phi1 \n",
    "    suff_stat=np.zeros(shape=(V,k))\n",
    "    suff_stat[wordid,]=phi1*np.array(wordcnt).reshape((-1,1))\n",
    "    return (gamma1,suff_stat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "built-vault",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure the two functions produce close results\n",
    "random.seed(1)\n",
    "N=5\n",
    "k=3\n",
    "V=6\n",
    "alpha=np.array([1,2,3])\n",
    "beta=np.random.randint(low=1,high=10,size=(3,4))\n",
    "beta=beta/np.sum(beta,axis=1).reshape((-1,1))\n",
    "word_dict=parsed_doc\n",
    "w=[1,2,0,3,0]\n",
    "res1=variational_inference(N,k,V,alpha,beta,w,conv_threshold=1e-5,max_iter=int(1e6))\n",
    "res2=e_step(N,k,V,alpha,beta,word_dict,conv_threshold=1e-5,max_iter=int(1e6))\n",
    "res1[0]-res2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "incredible-edinburgh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1[1]-res2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-findings",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "suspended-metabolism",
   "metadata": {},
   "source": [
    "def m_step(k,V,phi_star,w_star):\n",
    "    \"\"\"\n",
    "    M-step in variational EM, maximizes the lower bound on log-likelihood w.r.t. alpha and beta. (Section 5.3)\n",
    "    phi_star: M*Nd*k\n",
    "    w_star: M*Nd*V\n",
    "    \"\"\"\n",
    "    # update beta\n",
    "    for i in range(k):\n",
    "        for j in range(V):\n",
    "            beta[i,j]="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
