{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.special import psi  # gamma function utils\n",
    "from pprint import pprint\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "import logging\n",
    "import queue\n",
    "from numba import jit,njit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plain code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utils and Helper Class\n",
    "\n",
    "def tf(docs):\n",
    "    \"\"\"\n",
    "    This function is used to calculate the document-term matrix and id2word mapping\n",
    "    \"\"\"\n",
    "    # Clean up the text\n",
    "    docsc_clean = {}\n",
    "    total_term = []\n",
    "    for key, val in enumerate(docs):\n",
    "        val_clean = re.findall(r'[a-z]+', val.lower())\n",
    "        docsc_clean[f'd{key}'] = val_clean\n",
    "        total_term += val_clean\n",
    "\n",
    "    total_term_unique = sorted(set(total_term))\n",
    "    # change to list\n",
    "    # id2word = [(idx,word) for  idx, word in enumerate(total_term_unique)]\n",
    "    id2word = {idx: word for  idx, word in enumerate(total_term_unique)}\n",
    "\n",
    "    # Count the number of occurrences of term i in document j\n",
    "    for key, val in docsc_clean.items():\n",
    "        word_dir = dict.fromkeys(total_term_unique, 0)\n",
    "        for word in val:\n",
    "            word_dir[word] += 1\n",
    "        docsc_clean[key] = word_dir\n",
    "\n",
    "    tf_df = pd.DataFrame.from_dict(docsc_clean, orient='index')\n",
    "\n",
    "    return tf_df, id2word\n",
    "\n",
    "\n",
    "def dirichlet_expectation(sstats):\n",
    "    \"\"\"\n",
    "    For a vector theta ~ Dir(alpha), computes E[log(theta)] given alpha.\n",
    "    \"\"\"\n",
    "    if len(sstats.shape) == 1:\n",
    "        return psi(sstats) - psi(np.sum(sstats))\n",
    "    else:\n",
    "        return psi(sstats) - psi(np.sum(sstats, 1))[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "\n",
    "class LdaState:\n",
    "    def __init__(self, eta, shape, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : numpy.ndarray\n",
    "            The prior probabilities assigned to each term.\n",
    "        shape : tuple of (int, int)\n",
    "            Shape of the sufficient statistics: (number of topics to be found, number of terms in the vocabulary).\n",
    "        dtype : type\n",
    "            Overrides the numpy array default types.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta.astype(dtype, copy=False)\n",
    "        self.sstats = np.zeros(shape, dtype=dtype)\n",
    "        self.numdocs = 0\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def get_lambda(self):\n",
    "        \"\"\"Get the parameters of the posterior over the topics, also referred to as \"the topics\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Parameters of the posterior probability over topics.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.eta + self.sstats\n",
    "\n",
    "    def get_Elogbeta(self):\n",
    "        \"\"\"Get the log (posterior) probabilities for each topic.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Posterior probabilities for each topic.\n",
    "        \"\"\"\n",
    "        return dirichlet_expectation(self.get_lambda())\n",
    "\n",
    "    def blend(self, rhot, other, targetsize=None):\n",
    "        \"\"\"Merge the current state with another one using a weighted average for the sufficient statistics.\n",
    "\n",
    "        The number of documents is stretched in both state objects, so that they are of comparable magnitude.\n",
    "        This procedure corresponds to the stochastic gradient update from\n",
    "        `Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\"\n",
    "        <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_, see equations (5) and (9).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rhot : float\n",
    "            Weight of the `other` state in the computed average. A value of 0.0 means that `other`\n",
    "            is completely ignored. A value of 1.0 means `self` is completely ignored.\n",
    "        other : :class:`~gensim.models.ldamodel.LdaState`\n",
    "            The state object with which the current one will be merged.\n",
    "        targetsize : int, optional\n",
    "            The number of documents to stretch both states to.\n",
    "\n",
    "        \"\"\"\n",
    "        assert other is not None\n",
    "        if targetsize is None:\n",
    "            targetsize = self.numdocs\n",
    "\n",
    "        # stretch the current model's expected n*phi counts to target size\n",
    "        if self.numdocs == 0 or targetsize == self.numdocs:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0 * targetsize / self.numdocs\n",
    "        self.sstats *= (1.0 - rhot) * scale\n",
    "\n",
    "        # stretch the incoming n*phi counts to target size\n",
    "        if other.numdocs == 0 or targetsize == other.numdocs:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0 * targetsize / other.numdocs\n",
    "        self.sstats += rhot * scale * other.sstats\n",
    "        self.numdocs = targetsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions for my_lda_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalize(id2word,num_topics,dtype,random_state):\n",
    "    '''\n",
    "    initialize all the variables needed for LDA\n",
    "    '''\n",
    "    num_terms = len(id2word)\n",
    "\n",
    "    alpha = np.array( [1.0 / num_topics for i in range(num_topics)], dtype=dtype)\n",
    "\n",
    "    eta = np.array( [1.0 / num_topics for i in range(num_terms)], dtype=dtype)\n",
    "\n",
    "    rand  = np.random.RandomState(random_state)\n",
    "\n",
    "    model_states = LdaState(eta, (num_topics, num_terms), dtype=dtype)\n",
    "    model_states.sstats = rand.gamma(100., 1. / 100., (num_topics, num_terms))\n",
    "\n",
    "    expElogbeta = np.exp(dirichlet_expectation(model_states.sstats))\n",
    "    \n",
    "    return num_terms,alpha,eta,rand,model_states,expElogbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step_1(rand,chunk,num_topics, dtype,expElogbeta):\n",
    "    '''\n",
    "    e step \n",
    "    Initialize the variational distribution q(theta|gamma) for the chunk\n",
    "    '''\n",
    "    \n",
    "    gamma = rand.gamma(100., 1. / 100., (len(chunk), num_topics)).astype(dtype, copy=False)\n",
    "    tmpElogtheta = dirichlet_expectation(gamma)\n",
    "    tmpexpElogtheta = np.exp(tmpElogtheta)\n",
    "    sstats = np.zeros_like(expElogbeta, dtype=dtype)\n",
    "    converged = 0\n",
    "    \n",
    "    return gamma,tmpElogtheta,tmpexpElogtheta,sstats,converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step_2(chunk,gamma,tmpElogtheta,tmpexpElogtheta,expElogbeta,sstats,converged,dtype,iterations,alpha,gamma_threshold):\n",
    "    '''\n",
    "    e step continue\n",
    "    for each document d, update d's gamma and phi\n",
    "    '''\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    for d, doc in enumerate(chunk):\n",
    "        ids = [idx for idx, _ in doc]\n",
    "        cts = np.fromiter([cnt for _, cnt in doc], dtype=dtype, count=len(doc))\n",
    "        gammad = gamma[d, :]\n",
    "        Elogthetad = tmpElogtheta[d, :]\n",
    "        expElogthetad = tmpexpElogtheta[d, :]\n",
    "        expElogbetad = expElogbeta[:, ids]\n",
    "\n",
    "        # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\n",
    "        # phinorm is the normalizer.\n",
    "        phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
    "\n",
    "        gammad, expElogthetad,phinorm,converged = e_step_2_inner_update(iterations,gammad,alpha,expElogthetad,cts,phinorm,expElogbetad,gamma_threshold,converged,epsilon)\n",
    "        \n",
    "        gamma[d, :] = gammad\n",
    "        sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)\n",
    "    return gamma, sstats,converged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(model_states,pass_ ,num_updates, chunksize,other):\n",
    "    '''\n",
    "    m step\n",
    "    '''\n",
    "    previous_Elogbeta = model_states.get_Elogbeta()\n",
    "    rho = pow(1 + pass_ + (num_updates / chunksize), -0.5)\n",
    "    model_states.blend(rho, other)\n",
    "\n",
    "    current_Elogbeta = model_states.get_Elogbeta()\n",
    "    #Propagate the states topic probabilities to the inner object's attribute.\n",
    "    expElogbeta = np.exp(current_Elogbeta)\n",
    "\n",
    "    diff = np.mean(np.abs(previous_Elogbeta.ravel() - current_Elogbeta.ravel()))\n",
    "    num_updates += other.numdocs\n",
    "    \n",
    "    return model_states,num_updates,diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization on the 2 functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def e_step_2_inner_update(iterations,gammad,alpha,expElogthetad,cts,phinorm,expElogbetad,gamma_threshold,converged,epsilon):\n",
    "    '''\n",
    "    explicitly updating phi\n",
    "    '''\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        lastgamma = gammad\n",
    "        # We represent phi implicitly to save memory and time.\n",
    "        # Substituting the value of the optimal phi back into\n",
    "        # the update for gamma gives this update. Cf. Lee&Seung 2001.\n",
    "        gammad = (alpha + expElogthetad.astype(np.float32) * np.dot(cts.astype(np.float32) / phinorm.astype(np.float32), expElogbetad.T.astype(np.float32)))\n",
    "        Elogthetad = dirichlet_expectation_numba(gammad)\n",
    "        expElogthetad = np.exp(Elogthetad)\n",
    "        phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
    "        # If gamma hasn't changed much, we're done.\n",
    "        if np.mean(np.abs(gammad - lastgamma)) < gamma_threshold:\n",
    "            converged += 1\n",
    "            break\n",
    "\n",
    "    return gammad, expElogthetad,phinorm,converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main LDA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lda_func(corpus, num_topics, id2word, random_state=10,  passes=1, num_words=10,\n",
    "                iterations=50, gamma_threshold=0.001, dtype=np.float32,  chunksize=100, topics_only=True, verbose=False):\n",
    "    \n",
    "    \n",
    "    num_terms,alpha,eta,rand,model_states,expElogbeta = initalize(id2word,num_topics,dtype,random_state)\n",
    "\n",
    "    # Update\n",
    "    lencorpus = len(corpus)\n",
    "    chunksize = min(lencorpus, chunksize)\n",
    "    model_states.numdocs += lencorpus\n",
    "    num_updates = 0\n",
    "\n",
    "    for pass_ in range(passes):\n",
    "        all_chunks = chunks(corpus, chunksize)\n",
    "\n",
    "        for chunk_no, chunk in enumerate(all_chunks):\n",
    "            other = LdaState(eta, (num_topics, num_terms), dtype=dtype)\n",
    "            \n",
    "            if len(chunk) > 1:\n",
    "                if verbose:\n",
    "                    print(f'performing inference on a chunk of {len(chunk) } documents')\n",
    "            else:\n",
    "                raise\n",
    "            # e-step\n",
    "            gamma,tmpElogtheta,tmpexpElogtheta,sstats,converged = e_step_1(rand,chunk,num_topics, dtype,expElogbeta)\n",
    "\n",
    "            # e-step-2\n",
    "            gamma, sstats,converged = e_step_2(chunk,gamma,tmpElogtheta,tmpexpElogtheta,expElogbeta,sstats,converged,dtype,iterations,alpha,gamma_threshold)\n",
    "\n",
    "            if len(chunk) > 1:\n",
    "                if verbose:\n",
    "                    print(f\"{converged}/{len(chunk)} documents converged within {iterations} iterations\")\n",
    "\n",
    "            sstats *= expElogbeta\n",
    "\n",
    "            other.sstats += sstats\n",
    "            other.numdocs += gamma.shape[0]\n",
    "\n",
    "            # Do mstep\n",
    "            if verbose:\n",
    "                print('Update topics')\n",
    "            model_states, num_updates,diff = m_step(model_states,pass_ ,num_updates, chunksize,other)\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"topic diff {}\".format(diff))\n",
    "\n",
    "    shown = []\n",
    "    topic = model_states.get_lambda()\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        topic_ = topic[i]\n",
    "        topic_ = topic_ / topic_.sum()  # normalize to probability distribution\n",
    "        bestn = topic_.argsort()[-num_words:][::-1]\n",
    "\n",
    "        topic_ = [(id2word[id], topic_[id]) for id in bestn]\n",
    "        topic_ = ' + '.join('%.3f*\"%s\"' % (v, k) for k, v in topic_)\n",
    "        shown.append((i, topic_))\n",
    "\n",
    "    if topics_only:\n",
    "        return shown\n",
    "    else:\n",
    "        return shown,gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small dataset example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for analysis\n",
    "d1 = \"Java is a language for programming that develops a software for several platforms. A compiled code or bytecode on Java application can run on most of the operating systems including Linux, Mac operating system, and Linux. Most of the syntax of Java is derived from the C++ and C languages.\"\n",
    "d2 = \"Python supports multiple programming paradigms and comes up with a large standard library, paradigms included are object-oriented, imperative, functional and procedural.\"\n",
    "d3 = \"Go is typed statically compiled language. It was created by Robert Griesemer, Ken Thompson, and Rob Pike in 2009. This language offers garbage collection, concurrency of CSP-style, memory safety, and structural typing.\"\n",
    "d4 = \"A young girl when she first visited magical Underland, Alice Kingsleigh (Mia Wasikowska) is now a teenager with no memory of the place -- except in her dreams.\"\n",
    "d5 = \"Her life takes a turn for the unexpected when, at a garden party for her fiance and herself, she spots a certain white rabbit and tumbles down a hole after him. Reunited with her friends the Mad Hatter (Johnny Depp), the Cheshire Cat and others, Alice learns it is her destiny to end the Red Queen's (Helena Bonham Carter) reign of terror.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.038*\"and\" + 0.036*\"a\" + 0.026*\"the\" + 0.018*\"of\" + 0.017*\"her\" + '\n",
      "  '0.015*\"is\" + 0.013*\"for\" + 0.013*\"language\" + 0.013*\"paradigms\" + '\n",
      "  '0.011*\"with\"'),\n",
      " (1,\n",
      "  '0.030*\"a\" + 0.029*\"the\" + 0.025*\"of\" + 0.023*\"and\" + 0.022*\"is\" + '\n",
      "  '0.019*\"her\" + 0.017*\"for\" + 0.015*\"java\" + 0.013*\"with\" + 0.012*\"when\"')]\n"
     ]
    }
   ],
   "source": [
    "# Using slow version tf_df\n",
    "tf_df, id2word = tf([d1, d2, d3, d4, d5])\n",
    "\n",
    "lil = []\n",
    "for row in tf_df.values:\n",
    "    lil_sub = []\n",
    "    for idx, item in enumerate(row):\n",
    "        if item:\n",
    "            lil_sub.append((idx, item))\n",
    "    lil.append(lil_sub)\n",
    "    \n",
    "pprint(my_lda_func(corpus=lil, num_topics=2, id2word=id2word, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.7 ms ± 17.3 ms per loop (mean ± std. dev. of 3 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r3 -n2 my_lda_func(corpus=lil, num_topics=2, id2word=id2word, num_words=10)\n",
    "# without jit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.17 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "55.7 ms ± 30.4 ms per loop (mean ± std. dev. of 3 runs, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -r3 -n2 my_lda_func(corpus=lil, num_topics=2, id2word=id2word, num_words=10)\n",
    "# with jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world data (from Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Real world sample data\n",
    "raw_tweets = pd.read_csv('clean_tweets.csv')\n",
    "\n",
    "tweets_list = raw_tweets.Tweets.values.tolist()\n",
    "\n",
    "# Turn the list of string into a list of tokens\n",
    "clean_tweets = [t.split(',') for t in tweets_list]\n",
    "\n",
    "len(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(clean_tweets)\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in clean_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"thank\" + 0.007*\"go\" + 0.006*\"know\" + 0.006*\"year\" + 0.006*\"good\" + 0.006*\"time\" + 0.005*\"people\" + 0.005*\"look\" + 0.005*\"think\" + 0.005*\"need\"'),\n",
       " (1,\n",
       "  '0.010*\"more\" + 0.007*\"great\" + 0.007*\"go\" + 0.007*\"thank\" + 0.007*\"know\" + 0.006*\"love\" + 0.006*\"good\" + 0.006*\"day\" + 0.006*\"time\" + 0.006*\"new\"'),\n",
       " (2,\n",
       "  '0.010*\"thank\" + 0.010*\"look\" + 0.008*\"good\" + 0.007*\"go\" + 0.007*\"great\" + 0.007*\"love\" + 0.007*\"day\" + 0.006*\"new\" + 0.006*\"think\" + 0.006*\"come\"'),\n",
       " (3,\n",
       "  '0.007*\"people\" + 0.006*\"thank\" + 0.006*\"love\" + 0.006*\"new\" + 0.006*\"go\" + 0.006*\"time\" + 0.006*\"think\" + 0.005*\"great\" + 0.005*\"know\" + 0.005*\"look\"'),\n",
       " (4,\n",
       "  '0.014*\"more\" + 0.009*\"today\" + 0.008*\"go\" + 0.007*\"day\" + 0.006*\"think\" + 0.006*\"thank\" + 0.006*\"good\" + 0.006*\"love\" + 0.006*\"new\" + 0.005*\"year\"'),\n",
       " (5,\n",
       "  '0.008*\"thank\" + 0.008*\"good\" + 0.008*\"day\" + 0.006*\"think\" + 0.006*\"go\" + 0.006*\"work\" + 0.006*\"love\" + 0.006*\"need\" + 0.005*\"new\" + 0.005*\"year\"'),\n",
       " (6,\n",
       "  '0.009*\"good\" + 0.008*\"love\" + 0.008*\"time\" + 0.008*\"today\" + 0.007*\"new\" + 0.007*\"thank\" + 0.007*\"more\" + 0.007*\"go\" + 0.007*\"day\" + 0.006*\"think\"'),\n",
       " (7,\n",
       "  '0.011*\"good\" + 0.008*\"thank\" + 0.007*\"time\" + 0.007*\"new\" + 0.007*\"today\" + 0.007*\"go\" + 0.007*\"think\" + 0.006*\"great\" + 0.005*\"year\" + 0.005*\"need\"'),\n",
       " (8,\n",
       "  '0.012*\"good\" + 0.008*\"day\" + 0.008*\"thank\" + 0.007*\"like\" + 0.007*\"time\" + 0.007*\"video\" + 0.006*\"today\" + 0.006*\"look\" + 0.006*\"go\" + 0.006*\"year\"'),\n",
       " (9,\n",
       "  '0.010*\"good\" + 0.008*\"people\" + 0.008*\"think\" + 0.007*\"know\" + 0.007*\"go\" + 0.007*\"year\" + 0.006*\"great\" + 0.006*\"time\" + 0.006*\"thank\" + 0.005*\"say\"')]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lda_func(corpus=corpus, num_topics=10, id2word=id2word, num_words=10, chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus,\n",
    "                   id2word=id2word,\n",
    "                   num_topics=10, \n",
    "                   random_state=10,\n",
    "                   chunksize=100,\n",
    "#                    alpha='auto',```\n",
    "#                    per_word_topics=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4min 43s ± 0 ns per loop (mean ± std. dev. of 1 run, 2 loops each)\n",
      "3min 43s ± 0 ns per loop (mean ± std. dev. of 1 run, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "# original time comparison between plain code and genism's LDA\n",
    "%timeit -r1 -n2 my_lda_func(corpus=corpus, num_topics=10, id2word=id2word, num_words=10,chunksize=100)\n",
    "%timeit -r1 -n2 LdaModel(corpus=corpus,id2word=id2word,num_topics=10, random_state=10,chunksize=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5min 11s ± 0 ns per loop (mean ± std. dev. of 1 run, 2 loops each)\n",
      "4min 39s ± 0 ns per loop (mean ± std. dev. of 1 run, 2 loops each)\n"
     ]
    }
   ],
   "source": [
    "# new comparison between optimized and genism's LDA\n",
    "%timeit -r1 -n2 my_lda_func(corpus=corpus, num_topics=10, id2word=id2word, num_words=10,chunksize=100)\n",
    "%timeit -r1 -n2 LdaModel(corpus=corpus,id2word=id2word,num_topics=10, random_state=10,chunksize=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### before optimization stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          10610510 function calls (10610389 primitive calls) in 377.578 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 84 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000  377.578  377.578 {built-in method builtins.exec}\n",
      "        1    0.002    0.002  377.577  377.577 <string>:1(<module>)\n",
      "        1    0.478    0.478  377.575  377.575 <ipython-input-174-7f666848fa22>:1(my_lda_func)\n",
      "       60    3.199    0.053  369.852    6.164 <ipython-input-171-0bfea4aa5365>:1(e_step_2)\n",
      "     6000   25.189    0.004  365.232    0.061 <ipython-input-193-f2b2c4df11f4>:1(e_step_2_inner_update)\n",
      "1212106/1211986  291.012    0.000  321.753    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "   605842    1.785    0.000  291.408    0.000 <__array_function__ internals>:2(dot)\n",
      "   300102   14.794    0.000   28.073    0.000 <ipython-input-190-18a75ed24c34>:32(dirichlet_expectation)\n",
      "   299981    0.827    0.000   20.715    0.000 <__array_function__ internals>:2(mean)\n",
      "   299981    2.234    0.000   19.086    0.000 fromnumeric.py:3269(mean)\n",
      "   299981    9.138    0.000   16.853    0.000 _methods.py:143(_mean)\n",
      "   300102    0.908    0.000   13.279    0.000 <__array_function__ internals>:2(sum)\n",
      "   300102    2.200    0.000   11.316    0.000 fromnumeric.py:2105(sum)\n",
      "   300102    2.479    0.000    8.824    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "   600093    8.482    0.000    8.482    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "       60    1.451    0.024    6.887    0.115 <ipython-input-172-8ebfc2bda0e9>:1(m_step)\n",
      "      120    0.086    0.001    5.055    0.042 <ipython-input-190-18a75ed24c34>:77(get_Elogbeta)\n",
      "  1199805    4.813    0.000    4.813    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "   299981    2.478    0.000    2.950    0.000 _methods.py:59(_count_reduce_items)\n",
      "   299981    0.347    0.000    0.918    0.000 _asarray.py:86(asanyarray)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "profile = %prun -r -q my_lda_func(corpus=corpus, num_topics=10, id2word=id2word, num_words=10,chunksize=100)\n",
    "profile.sort_stats('cumtime').print_stats(20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### after optimization stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          10753333 function calls (10648551 primitive calls) in 271.575 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 1345 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000  271.576  271.576 {built-in method builtins.exec}\n",
      "        1    0.002    0.002  271.576  271.576 <string>:1(<module>)\n",
      "        1    0.358    0.358  271.573  271.573 <ipython-input-174-7f666848fa22>:1(my_lda_func)\n",
      "       60    3.256    0.054  265.139    4.419 <ipython-input-171-0bfea4aa5365>:1(e_step_2)\n",
      "12000/6000   28.358    0.002  259.502    0.043 <ipython-input-191-ecfb2892f538>:3(e_step_2_inner_update)\n",
      "1212106/1211986  181.814    0.000  210.188    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "   605842    1.720    0.000  182.283    0.000 <__array_function__ internals>:2(dot)\n",
      "   300102   12.938    0.000   24.892    0.000 <ipython-input-190-18a75ed24c34>:32(dirichlet_expectation)\n",
      "   299981    0.751    0.000   19.553    0.000 <__array_function__ internals>:2(mean)\n",
      "   299981    2.067    0.000   18.058    0.000 fromnumeric.py:3269(mean)\n",
      "   299981    8.674    0.000   15.991    0.000 _methods.py:143(_mean)\n",
      "   300102    0.921    0.000   11.954    0.000 <__array_function__ internals>:2(sum)\n",
      "   300102    1.987    0.000   10.050    0.000 fromnumeric.py:2105(sum)\n",
      "        3    0.000    0.000    9.655    3.218 dispatcher.py:337(_compile_for_args)\n",
      "     60/3    0.010    0.000    9.652    3.217 compiler_lock.py:29(_acquire_compile_lock)\n",
      "      6/4    0.000    0.000    9.612    2.403 compiler.py:380(_compile_core)\n",
      "      6/4    0.001    0.000    9.610    2.402 compiler_machinery.py:318(run)\n",
      "    57/37    0.005    0.000    9.608    0.260 compiler_machinery.py:259(_runPass)\n",
      "  170/110    0.001    0.000    9.601    0.087 compiler_machinery.py:263(check)\n",
      "        5    0.001    0.000    9.395    1.879 object_mode_passes.py:114(run_pass)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# after optimization\n",
    "profile = %prun -r -q my_lda_func(corpus=corpus, num_topics=10, id2word=id2word, num_words=10,chunksize=100)\n",
    "profile.sort_stats('cumtime').print_stats(20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
