{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weekly-technician",
   "metadata": {},
   "source": [
    "# The Variational EM algorithm of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import collections\n",
    "import random\n",
    "from scipy.special import gammaln, psi, polygamma\n",
    "from functools import reduce\n",
    "from warnings import warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of parsing a document\n",
    "doc='two-three one, Sth*, five, else ^#%$@#@#'\n",
    "vocab=dict(zip(('one','two','three','sth','four','else'),range(6)))\n",
    "parsed_doc=utilities.parse_doc(doc,vocab)\n",
    "parsed_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-injection",
   "metadata": {},
   "source": [
    "## The equivalent class of words\n",
    "Suppose we have a single document. Let \n",
    "\n",
    "$$\\tilde{w_n}=\\text{j such that }w_n^j=1$$\n",
    "To carry out the inference, first note that for fixed $i,j$, $\\{\\phi_{ni}: \\tilde{w_n}=j\\}$ will all be the same. This suggests us to define the following quantity:\n",
    "\n",
    "$$\\tilde{\\phi_{ji}}=\\phi_{ni}, \\text{ where }\\tilde{w_n}=j$$\n",
    "We only need to update $\\tilde{\\phi}$ in the variational inference (E-step). For fixed $i$, the length of $\\tilde{\\phi}_{:,i}$ is the cardinality of $\\{\\tilde{w_n}: n=1,\\cdots, N\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_inference(N,k,V,alpha,beta,w,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Variational inference algorithm for document-specific parameters of a single doc in LDA (figure 6 in the paper)\n",
    "    Arguments:\n",
    "    N: number of words in the document\n",
    "    k: number of topics\n",
    "    V: length of vocabulary\n",
    "    alpha: corpus-level Dirichlet parameter, k-vector\n",
    "    beta: corpus-level multinomial parameter, k * V matrix\n",
    "    w: word id obtained from parsing the document\n",
    "    conv_threshold: threshold for convergence\n",
    "    max_iter: maximum number of iterations\n",
    "    Output:\n",
    "    A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "    First element: $\\gamma^*$, k-vector\n",
    "    Second element: the second sum in Eq(9), k*V matrix\n",
    "    \"\"\"\n",
    "    phi0=np.full(shape=(N,k),fill_value=1/k) \n",
    "    phi1=phi0\n",
    "    gamma0=alpha+N/k\n",
    "    for it in range(max_iter):\n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                phi1[n,i]=beta[i,w[n]]*np.exp(psi(gamma0[i]))\n",
    "            phi1[n,]=phi1[n,]/np.sum(phi1[n,])\n",
    "        gamma1=alpha+np.sum(phi1,axis=0)\n",
    "        # stop if gamma has converged\n",
    "        if np.mean(np.abs(gamma0-gamma1))<conv_threshold:\n",
    "            break\n",
    "        gamma0=gamma1\n",
    "        phi0=phi1 \n",
    "    suff_stat=np.zeros(shape=(V,k))\n",
    "    for n in range(N):\n",
    "        suff_stat[w[n],]=suff_stat[w[n],]+phi1[n,]\n",
    "    return (gamma1,suff_stat.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(N,k,V,alpha,beta,word_dict,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Variational inference algorithm for document-specific parameters of a single doc in LDA with the equivalent class representation.\n",
    "    Arguments:\n",
    "    N: number of words\n",
    "    k: number of topics\n",
    "    V: length of vocabulary\n",
    "    alpha: corpus-level Dirichlet parameter, k-vector\n",
    "    beta: corpus-level multinomial parameter, k * V matrix\n",
    "    word_dict: word_dict from parse_doc\n",
    "    conv_threshold: threshold for convergence\n",
    "    max_iter: maximum number of iterations\n",
    "    Output:\n",
    "    A tuple of document specific optimizing parameters $(\\gamma^*, \\phi^*)$ obtained from variational inference.  \n",
    "    First element: $\\gamma^*$, k-vector\n",
    "    Second element: the second sum in Eq(9), k*V matrix\n",
    "    \"\"\"\n",
    "    conv=False\n",
    "    wordid=list(word_dict.keys())\n",
    "    wordcnt=list(word_dict.values())\n",
    "    phi0=np.full(shape=(len(wordid),k),fill_value=1/k) # phi_tilde \n",
    "    phi1=phi0\n",
    "    gamma0=alpha+N/k\n",
    "    for it in range(max_iter):\n",
    "        for j in range(len(wordid)):\n",
    "            for i in range(k):\n",
    "                phi1[j,i]=beta[i,wordid[j]]*np.exp(psi(gamma0[i]))*wordcnt[j]\n",
    "            phi1[j,]=phi1[j,]/np.sum(phi1[j,])\n",
    "        gamma1=alpha+np.sum(phi1*np.array(wordcnt).reshape((-1,1)),axis=0)\n",
    "        # stop if gamma has converged\n",
    "        if np.mean(np.abs(gamma0-gamma1))<conv_threshold:\n",
    "            conv=True\n",
    "            break\n",
    "        gamma0=gamma1\n",
    "        phi0=phi1 \n",
    "    if not conv:\n",
    "        warn('Variational inference has not converged. Try more iterations.')\n",
    "    suff_stat=np.zeros(shape=(V,k))\n",
    "    suff_stat[wordid,]=phi1*np.array(wordcnt).reshape((-1,1))\n",
    "    return (gamma1,suff_stat.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the two functions produce close results\n",
    "random.seed(1)\n",
    "N=5\n",
    "k=3\n",
    "V=6\n",
    "alpha=np.array([1,2,3])\n",
    "beta=np.random.randint(low=1,high=10,size=(3,6))\n",
    "beta=beta/np.sum(beta,axis=1).reshape((-1,1))\n",
    "word_dict=parsed_doc\n",
    "w=[1,2,0,4,0]\n",
    "res1=variational_inference(N,k,V,alpha,beta,w,conv_threshold=1e-5)\n",
    "res2=e_step(N,k,V,alpha,beta,word_dict,conv_threshold=1e-5)\n",
    "res1[0]-res2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1[1]-res2[1]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sublime-mobility",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "chemical-intent",
   "metadata": {},
   "source": [
    "## The M-step\n",
    "\n",
    "The parameters $\\beta$ and $\\alpha$ are updated in the M step. \n",
    "\n",
    "$\\beta$ is updated with Eq(9). \n",
    "\n",
    "$\\alpha$ is updated with Newton-Raphson:\n",
    "$$\\alpha_{new}=\\alpha_{old}-H(\\alpha_{old})^{-1}g(\\alpha_{old}),$$\n",
    "where $H(\\alpha)=(\\frac{\\partial^2\\mathcal{L}}{\\partial\\alpha_i\\partial\\alpha_j})_{k\\times k}$ is the Hessian matrix and $g(\\alpha)=(\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i})_{i=1}^k$ is the gradient. \n",
    "\n",
    "A.4.2 shows that \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\alpha_i}=M \\left(\\Psi\\left(\\sum_{j=1}^k \\alpha_j\\right)-\\Psi(\\alpha_i)\\right) + \\sum_{d=1}^M \\left(\\Psi(\\gamma_{di})-\\Psi(\\sum_{j=1}^k\\gamma_{dj}) \\right),$$\n",
    "$$\\frac{\\partial^2\\mathcal{L}}{\\partial\\alpha_i\\partial\\alpha_j}=\\delta(i,j)M\\Psi'(\\alpha_i)-\\Psi'\\left(\\sum_{j=1}^k\\alpha_j\\right),$$\n",
    "i.e.,\n",
    "$$H(\\alpha)=diag(h)+1z1^T,$$\n",
    "where $$z=-\\Psi'\\left(\\sum_{j=1}^k\\alpha_j\\right),h=M\\Psi'(\\alpha)$$\n",
    "By A.2, we have \n",
    "$$(H^{-1}g)_i=\\frac{g_i-c}{h_i},$$\n",
    "where \n",
    "$$c=\\frac{\\sum_{j=1}^k g_j/h_j}{1/z+\\sum_{j=1}^k h_j^{-1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    M-step in variational EM, maximizing the lower bound on log-likelihood w.r.t. alpha and beta. (Section 5.3)\n",
    "    Arguments:\n",
    "    M: number of documents in the corpus\n",
    "    k: number of topics\n",
    "    V: length of vocab\n",
    "    suff_stat_list: M-list of sufficient statistics (k * V matrices), one for each doc\n",
    "    gamma_list: M-list of gamma's (k-vectors), one for each doc\n",
    "    conv_threshold: convergence threshold in Newton-Raphson\n",
    "    max_iter: maximum number of iterations in Newton-Raphson\n",
    "    Output:\n",
    "    A 2-tuple. \n",
    "    First element: beta (k*V matrix)\n",
    "    Second element: alpha (k*1)\n",
    "    \"\"\"\n",
    "    conv=False\n",
    "    # update beta\n",
    "    beta=reduce(lambda x,y: x+y, suff_stat_list)\n",
    "    beta=beta/np.sum(beta,axis=1).reshape((-1,1))\n",
    "    # update alpha (Newton-Raphson)\n",
    "    alpha0=np.full((k,1),fill_value=1/k)\n",
    "    for it in range(max_iter):\n",
    "        psi_sum_alpha=psi(np.sum(alpha0))\n",
    "        psi_sum_gamma=np.array(list(map(lambda x: psi(np.sum(x)),gamma_list))).reshape((M,1)) # M*1 \n",
    "        psi_gamma=psi(np.array(gamma_list)) # M*k matrix\n",
    "        g=M*(psi_sum_alpha-psi(alpha0)).reshape((k,1))- np.sum(psi_gamma-psi_sum_gamma,axis=0).reshape((k,1)) # k*1 \n",
    "        h=M*polygamma(1,alpha0)\n",
    "        z=-polygamma(1,np.sum(alpha0))\n",
    "        c=np.sum(g/h.reshape((k,1)))/(1/z+np.sum(1/h))\n",
    "        invHg=(g-c)/h.reshape((k,1))\n",
    "        alpha1=alpha0-invHg+1e-10 # 1e-10 is added for numerical stability\n",
    "        if np.max(np.abs(alpha1-alpha0))<conv_threshold:\n",
    "            #print('finished at iteration',it)\n",
    "            conv=True\n",
    "            break\n",
    "        alpha0=alpha1\n",
    "    if not conv:\n",
    "        warn('Newton-Raphson has not converged. Try more iterations.')\n",
    "    return (beta,alpha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example\n",
    "M=10\n",
    "suff_stat_list=[res2[1]]*M\n",
    "gamma_list=[res2[0]]*M\n",
    "res3=m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(100))\n",
    "res4=m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(1e4))\n",
    "res3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-purpose",
   "metadata": {},
   "source": [
    "## The variational EM without smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_em(Nd,alpha0,beta0,word_dicts,vocab,M,k, conv_threshold=1e-5,max_iter=int(1e6)):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    Nd: list of length of documents \n",
    "    alpha0: initialization of alpha\n",
    "    beta0: initialization of beta\n",
    "    word_dicts: list of word_dict of documents, in the same order as N\n",
    "    vocab: vocabulary\n",
    "    M: number of documents\n",
    "    k: number of topics\n",
    "    \"\"\"\n",
    "    V=len(vocab)\n",
    "    #\n",
    "    wordid=list(word_dicts[0].keys())\n",
    "    wordcnt=list(word_dicts[0].values())\n",
    "    gamma0=alpha0+N/k\n",
    "    #\n",
    "    for it in range(max_iter):\n",
    "        print(it)\n",
    "        e_estimates=list(map(lambda x,y: e_step(x,k,V,alpha0,beta0,y,conv_threshold=1e-5,max_iter=int(1e6)), Nd,word_dicts))\n",
    "        gamma_list=list(map(lambda x:x[0],e_estimates))\n",
    "        suff_stat_list=list(map(lambda x:x[1],e_estimates))\n",
    "        m_estimates=m_step(M,k,V,suff_stat_list,gamma_list,conv_threshold=1e-5,max_iter=int(1e6))\n",
    "        alpha1=m_estimates[1]\n",
    "        beta1=m_estimates[0]\n",
    "        if np.max(np.abs(beta1-beta0))<conv_threshold:\n",
    "            break\n",
    "        alpha0=alpha1.reshape(k)\n",
    "        beta0=beta1\n",
    "    return (alpha0,beta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "M=2\n",
    "variational_em([N,N],alpha,beta,[word_dict,word_dict],vocab,M,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "es=list(variational_em([N,N],alpha,beta,[word_dict,word_dict],vocab,M,k))[0]variational_em([N,N],alpha,beta,[word_dict,word_dict],vocab,M,k)\n",
    "aaa=np.array([1,2,3]).reshape((3,1))\n",
    "aaa.reshape(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-bowling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(x):\n",
    "    return x+['s']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test2(x):\n",
    "    x=x*10\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=['a']\n",
    "test2(x)\n",
    "print(x)\n",
    "x=test2(x)\n",
    "test1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_em([N,N],alpha,beta,[word_dict,word_dict],vocab,M,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-first",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
